{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **NLP Introduction & Text Processing**"
      ],
      "metadata": {
        "id": "I3TCK2GBGm6w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **What is Computational Linguistics and how does it relate to NLP?**\n",
        "\n",
        "  Ans. Computational linguistics and Natural Language Processing (NLP) are closely related fields that focus on the interaction between computers and human language.\n",
        "\n",
        "**Computational Linguistics** (CL) is an interdisciplinary field that combines linguistics, computer science, artificial intelligence, and cognitive science. It focuses on understanding the linguistic rules and structures of human language and developing computational models to process and analyze language. CL is more theoretical and aims to build models of language for linguistic research.\n",
        "\n",
        "**Natural Language Processing** (NLP) is a subfield of artificial intelligence that focuses on enabling computers to understand, interpret, and manipulate human language. NLP is more practical and aims to develop applications that involve language, such as machine translation, sentiment analysis, text summarization, and chatbots.\n",
        "\n",
        "**Relationship:** NLP relies heavily on the theories and models developed in computational linguistics. CL provides the theoretical foundation and computational frameworks for NLP tasks. In essence, computational linguistics is the science behind NLP, and NLP is the engineering that puts CL theories into practice."
      ],
      "metadata": {
        "id": "gxSMMrarGsUf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **Briefly describe the historical evolution of Natural Language Processing.**\n",
        "\n",
        "  Ans. The historical evolution of Natural Language Processing (NLP) can be broadly categorized into several periods:\n",
        "\n",
        "*   **Early Years (1950s-1960s):** This period was characterized by rule-based approaches and machine translation efforts. Early systems relied on hand-crafted rules and dictionaries to process language. The Georgetown-IBM experiment in 1954 is a notable example, aiming to automatically translate Russian to English.\n",
        "\n",
        "*   **Statistical Revolution (Late 1980s-Early 2000s):** The rise of machine learning and the availability of larger datasets led to a shift towards statistical methods. Techniques like Hidden Markov Models (HMMs) and Support Vector Machines (SVMs) became prominent for tasks like part-of-speech tagging and parsing.\n",
        "\n",
        "*   **Machine Learning and Feature Engineering (Early 2000s-2010s):** This era saw a focus on feature engineering, where researchers manually designed features from text to train machine learning models. This period also saw the development of techniques like Conditional Random Fields (CRFs) and the increasing use of large annotated corpora.\n",
        "\n",
        "*   **Deep Learning Era (2010s-Present):** The advent of deep learning, particularly neural networks, revolutionized NLP. Architectures like Recurrent Neural Networks (RNNs), Convolutional Neural Networks (CNNs), and later, Transformers, achieved state-of-the-art results on various NLP tasks, leading to significant progress in areas like machine translation, text generation, and question answering.\n",
        "\n",
        "*   **Large Language Models (LLMs) (Late 2010s-Present):** The development of massive pre-trained language models like BERT, GPT, and others marked a new era in NLP. These models, trained on vast amounts of text data, can be fine-tuned for a wide range of downstream tasks with remarkable performance, pushing the boundaries of what's possible with NLP."
      ],
      "metadata": {
        "id": "bHR3WnKaG23-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **List and explain three major use cases of NLP in today’s tech industry.**\n",
        "\n",
        "  Ans. Here are three major use cases of Natural Language Processing (NLP) in today's tech industry:\n",
        "\n",
        "1.  **Sentiment Analysis:** This involves determining the emotional tone behind a piece of text. Companies use sentiment analysis to understand customer feedback from social media, reviews, and surveys. This helps in gauging public opinion, identifying areas for improvement, and making data-driven decisions. For example, a company can analyze tweets about their product to see if the overall sentiment is positive, negative, or neutral.\n",
        "\n",
        "2.  **Machine Translation:** NLP enables the automatic translation of text or speech from one language to another. This is widely used in applications like Google Translate, which allows people to communicate across language barriers. It's also crucial for businesses operating globally, enabling them to localize content and communicate with customers and partners in different languages.\n",
        "\n",
        "3.  **Chatbots and Virtual Assistants:** NLP powers conversational AI agents like chatbots and virtual assistants (e.g., Siri, Alexa). These systems use NLP to understand user queries in natural language and provide relevant responses or perform tasks. They are used for customer service, providing information, automating tasks, and enhancing user interaction with technology."
      ],
      "metadata": {
        "id": "Ojo9Q7dSHAqu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **What is text normalization and why is it essential in text processing tasks?**\n",
        "\n",
        "  Ans. **Text normalization** is the process of converting text into a canonical (standard) form. The goal is to reduce the variations in text so that different forms of the same word or concept are treated consistently during processing. This is crucial because raw text data is often noisy and inconsistent, with variations in spelling, capitalization, punctuation, and word forms (e.g., \"run,\" \"running,\" \"ran\").\n",
        "\n",
        "**Why it is essential in text processing tasks:**\n",
        "\n",
        "Text normalization is essential for several reasons:\n",
        "\n",
        "*   **Reduces redundancy:** It helps in treating different variations of the same word as a single entity, reducing the size of the vocabulary and simplifying further analysis.\n",
        "*   **Improves accuracy:** By standardizing the text, it improves the accuracy of NLP tasks such as information retrieval, text classification, and sentiment analysis. For example, if we're searching for documents about \"fishing,\" normalizing \"fishes\" and \"fished\" to their base form \"fish\" will ensure us to capture all relevant documents.\n",
        "*   **Facilitates analysis:** Normalized text is easier to process and analyze with algorithms and models. It ensures that the models are not confused by variations in the text data.\n",
        "*   **Enhances comparability:** It allows for easier comparison and analysis of different text documents or datasets.\n",
        "\n",
        "Common text normalization techniques include:\n",
        "\n",
        "*   **Lowercasing:** Converting all text to lowercase to treat words like \"The\" and \"the\" the same.\n",
        "*   **Punctuation removal:** Removing punctuation marks that may not be relevant for analysis.\n",
        "*   **Tokenization:** Splitting text into individual words or tokens.\n",
        "*   **Stemming:** Reducing words to their root form (e.g., \"running\" -> \"run\"). This is a cruder approach than lemmatization.\n",
        "*   **Lemmatization:** Reducing words to their base or dictionary form (e.g., \"better\" -> \"good\"). This is usually more sophisticated than stemming as it considers the context and meaning of the word.\n",
        "*   **Handling special characters and numbers:** Deciding how to treat numbers, symbols, and other special characters.\n",
        "\n",
        "In summary, text normalization is a foundational step in most text processing pipelines, ensuring that the text data is clean, consistent, and ready for further analysis and modeling."
      ],
      "metadata": {
        "id": "dsmHnQCaHInN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. **Compare and contrast stemming and lemmatization with suitable examples.**\n",
        "\n",
        "  Ans. **Stemming** and **Lemmatization** are both techniques used in text normalization to reduce words to their base form. However, they differ in their approach and the resulting form.\n",
        "\n",
        "**Stemming:**\n",
        "\n",
        "*   **Approach:** Stemming is a more rudimentary process that simply chops off the end of a word to arrive at a root form. It's often rule-based and doesn't consider the context or meaning of the word.\n",
        "*   **Result:** The resulting \"stem\" may not be a valid word in the dictionary.\n",
        "*   **Speed:** Generally faster than lemmatization.\n",
        "*   **Accuracy:** Less accurate than lemmatization as it can result in meaningless stems.\n",
        "*   **Example:**\n",
        "    *   \"running\" -> \"run\"\n",
        "    *   \"fishes\" -> \"fish\"\n",
        "    *   \"studies\" -> \"studi\" (not a valid word)\n",
        "    *   \"better\" -> \"better\" (doesn't handle irregular forms)\n",
        "\n",
        "**Lemmatization:**\n",
        "\n",
        "*   **Approach:** Lemmatization is a more sophisticated process that uses a dictionary and linguistic rules to return the base or dictionary form of a word, known as the \"lemma.\" It considers the context of the word to determine the correct lemma.\n",
        "*   **Result:** The resulting \"lemma\" is a valid word in the dictionary.\n",
        "*   **Speed:** Generally slower than stemming due to the dictionary lookup and linguistic analysis.\n",
        "*   **Accuracy:** More accurate than stemming as it produces valid words and handles irregular forms.\n",
        "*   **Example:**\n",
        "    *   \"running\" -> \"run\"\n",
        "    *   \"fishes\" -> \"fish\"\n",
        "    *   \"studies\" -> \"study\" (valid word)\n",
        "    *   \"better\" -> \"good\" (handles irregular forms)\n",
        "\n",
        "**Comparison and Contrast:**\n",
        "\n",
        "| Feature      | Stemming                         | Lemmatization                      |\n",
        "| :----------- | :------------------------------- | :--------------------------------- |\n",
        "| **Approach** | Rule-based, chops off ends       | Dictionary and linguistic rules    |\n",
        "| **Result**   | May not be a valid word (stem)   | Valid word (lemma)                 |\n",
        "| **Speed**    | Faster                           | Slower                             |\n",
        "| **Accuracy** | Less accurate                    | More accurate                      |\n",
        "| **Context**  | Doesn't consider context         | Considers context                  |\n",
        "| **Irregular Forms** | Doesn't handle irregular forms | Handles irregular forms (e.g., \"better\" -> \"good\") |\n",
        "\n",
        "In summary, stemming is a faster but less accurate method that produces word stems that may not be valid words. Lemmatization is a slower but more accurate method that produces valid word lemmas by considering context and using a dictionary. The choice between stemming and lemmatization depends on the specific NLP task and the desired level of accuracy and speed."
      ],
      "metadata": {
        "id": "LNSP6D7tHTX3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. **Write a Python program that uses regular expressions (regex) to extract all email addresses from the following block of text:**\n",
        "\n",
        "    “Hello team, please contact us at support@xyz.com for technical issues, or reach out to our HR at hr@xyz.com. You can also connect with John at john.doe@xyz.org and jenny via jenny_clarke126@mail.co.us. For partnership inquiries, email partners@xyz.biz.”"
      ],
      "metadata": {
        "id": "lC-fIV8JHhz1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e98a5002",
        "outputId": "91161476-55bc-46b0-d0cf-84fe438b3e9d"
      },
      "source": [
        "import re\n",
        "\n",
        "text = \"Hello team, please contact us at support@xyz.com for technical issues, or reach out to our HR at hr@xyz.com. You can also connect with John at john.doe@xyz.org and jenny via jenny_clarke126@mail.co.us. For partnership inquiries, email partners@xyz.biz.\"\n",
        "\n",
        "# Regex pattern for email addresses\n",
        "email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
        "\n",
        "# Find all email addresses in the text\n",
        "email_addresses = re.findall(email_pattern, text)\n",
        "\n",
        "# Print the extracted email addresses\n",
        "for email in email_addresses:\n",
        "    print(email)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "support@xyz.com\n",
            "hr@xyz.com\n",
            "john.doe@xyz.org\n",
            "jenny_clarke126@mail.co.us\n",
            "partners@xyz.biz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. **Given the sample paragraph below, perform string tokenization and frequency distribution using Python and NLTK:**\n",
        "\n",
        "     “Natural Language Processing (NLP) is a fascinating field that combines linguistics, computer science, and artificial intelligence. It enables machines to understand, interpret, and generate human language. Applications of NLP include chatbots, sentiment analysis, and machine translation. As technology advances, the role of NLP in modern solutions is becoming increasingly critical.” **"
      ],
      "metadata": {
        "id": "Z2Zf9nT_H2xO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c24c2199",
        "outputId": "2c7cd88b-b2fa-476c-967f-48739a7d107a"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7560da78",
        "outputId": "65400cf8-0ae9-4a1b-8c04-b5fc072dc066"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "text = \"Natural Language Processing (NLP) is a fascinating field that combines linguistics, computer science, and artificial intelligence. It enables machines to understand, interpret, and generate human language. Applications of NLP include chatbots, sentiment analysis, and machine translation. As technology advances, the role of NLP in modern solutions is becoming increasingly critical.\"\n",
        "\n",
        "# Tokenization\n",
        "tokens = word_tokenize(text)\n",
        "print(\"Tokens:\")\n",
        "print(tokens)\n",
        "\n",
        "# Frequency Distribution\n",
        "fdist = FreqDist(tokens)\n",
        "print(\"\\nFrequency Distribution:\")\n",
        "print(fdist.most_common(10)) # Display the 10 most common tokens"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:\n",
            "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', 'that', 'combines', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', '.', 'It', 'enables', 'machines', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', '.', 'Applications', 'of', 'NLP', 'include', 'chatbots', ',', 'sentiment', 'analysis', ',', 'and', 'machine', 'translation', '.', 'As', 'technology', 'advances', ',', 'the', 'role', 'of', 'NLP', 'in', 'modern', 'solutions', 'is', 'becoming', 'increasingly', 'critical', '.']\n",
            "\n",
            "Frequency Distribution:\n",
            "[(',', 7), ('.', 4), ('NLP', 3), ('and', 3), ('is', 2), ('of', 2), ('Natural', 1), ('Language', 1), ('Processing', 1), ('(', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. **Create a custom annotator using spaCy or NLTK that identifies and labels proper nouns in a given text.**"
      ],
      "metadata": {
        "id": "mCuIrqGEILiJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "b79a170a",
        "outputId": "cbeb79e3-123f-491e-804d-cd8285d85735"
      },
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.10)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.10.5)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (2.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e865268",
        "outputId": "ac22e78f-2bbe-464f-c48b-ae343e93d265"
      },
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"Natural Language Processing (NLP) is a fascinating field that combines linguistics, computer science, and artificial intelligence. It enables machines to understand, interpret, and generate human language. Applications of NLP include chatbots, sentiment analysis, and machine translation. As technology advances, the role of NLP in modern solutions is becoming increasingly critical.\"\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Identify and label proper nouns\n",
        "proper_nouns = [(token.text, token.pos_) for token in doc if token.pos_ == \"PROPN\"]\n",
        "\n",
        "# Print the proper nouns\n",
        "print(\"Proper Nouns:\")\n",
        "for noun, pos in proper_nouns:\n",
        "    print(f\"{noun}: {pos}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proper Nouns:\n",
            "Natural: PROPN\n",
            "Language: PROPN\n",
            "Processing: PROPN\n",
            "NLP: PROPN\n",
            "NLP: PROPN\n",
            "NLP: PROPN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. **Using Genism, demonstrate how to train a simple Word2Vec model on the following dataset consisting of example sentences:**\n",
        "\n",
        "    dataset = [ \"Natural language processing enables computers to understand human language\", \"Word embeddings are a type of word representation that allows words with similar meaning to have similar representation\", \"Word2Vec is a popular word embedding technique used in many NLP applications\", \"Text preprocessing is a critical step before training word embeddings\", \"Tokenization and normalization help clean raw text for modeling\" ]\n",
        "    \n",
        "    **Write code that tokenizes the dataset, preprocesses it, and trains a Word2Vec model using Gensim.**"
      ],
      "metadata": {
        "id": "agOC50zEIiOG"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75c76623",
        "outputId": "11995612-9e12-47ef-ec22-f614d79abbb0"
      },
      "source": [
        "!pip install gensim"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.2)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.4.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.0)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "243dc4c1",
        "outputId": "96500749-3425-4050-e653-5d673a44d961"
      },
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "# Download necessary NLTK data (if not already downloaded)\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except nltk.downloader.DownloadError:\n",
        "    nltk.download('punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "\n",
        "dataset = [\n",
        "    \"Natural language processing enables computers to understand human language\",\n",
        "    \"Word embeddings are a type of word representation that allows words with similar meaning to have similar representation\",\n",
        "    \"Word2Vec is a popular word embedding technique used in many NLP applications\",\n",
        "    \"Text preprocessing is a critical step before training word embeddings\",\n",
        "    \"Tokenization and normalization help clean raw text for modeling\"\n",
        "]\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "    # Remove punctuation and numbers\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens\n",
        "\n",
        "# Preprocess the dataset\n",
        "processed_dataset = [preprocess_text(sentence) for sentence in dataset]\n",
        "\n",
        "# Train the Word2Vec model\n",
        "# vector_size: Dimensionality of the word vectors.\n",
        "# window: The maximum distance between the current and predicted word within a sentence.\n",
        "# min_count: Ignores all words with total frequency lower than this.\n",
        "# sg: Training algorithm: 1 for skip-gram; 0 for CBOW.\n",
        "model = Word2Vec(sentences=processed_dataset, vector_size=100, window=5, min_count=1, workers=4, sg=0)\n",
        "\n",
        "# Print the trained model's vocabulary size\n",
        "print(f\"Vocabulary size: {len(model.wv)}\")\n",
        "\n",
        "# Example: Get the vector for a word\n",
        "word_vector = model.wv['language']\n",
        "print(f\"\\nVector for 'language':\\n{word_vector[:10]}...\") # Print first 10 elements\n",
        "\n",
        "# Example: Find most similar words\n",
        "similar_words = model.wv.most_similar('language')\n",
        "print(f\"\\nWords similar to 'language':\")\n",
        "for word, similarity in similar_words:\n",
        "    print(f\"{word}: {similarity:.4f}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 46\n",
            "\n",
            "Vector for 'language':\n",
            "[-0.00958061  0.00894419  0.00416531  0.00923353  0.00664613  0.00292132\n",
            "  0.00980621 -0.004423   -0.0067969   0.00421717]...\n",
            "\n",
            "Words similar to 'language':\n",
            "technique: 0.2854\n",
            "text: 0.1991\n",
            "processing: 0.1907\n",
            "are: 0.1001\n",
            "step: 0.0966\n",
            "is: 0.0747\n",
            "that: 0.0728\n",
            "representation: 0.0608\n",
            "meaning: 0.0468\n",
            "modeling: 0.0448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. **Imagine you are a data scientist at a fintech startup. You’ve been tasked with analyzing customer feedback. Outline the steps you would take to clean, process, and extract useful insights using NLP techniques from thousands of customer reviews.**"
      ],
      "metadata": {
        "id": "3mNy7ld-I9FT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8e2ab90",
        "outputId": "e60af7aa-a8b2-4f81-d002-596eadb925b0"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import NMF\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import spacy\n",
        "\n",
        "# --- Step 1: Data Loading and Initial Inspection ---\n",
        "# In a real scenario, we would load our data from a file (e.g., CSV, JSON)\n",
        "# For this example, we'll use a sample list of reviews.\n",
        "# Replace this with our actual data loading code.\n",
        "customer_reviews = [\n",
        "    \"The product is great! I love the features.\",\n",
        "    \"This app is very slow and crashes frequently.\",\n",
        "    \"Customer service was helpful and resolved my issue quickly.\",\n",
        "    \"The user interface is confusing and hard to navigate.\",\n",
        "    \"Amazing experience, highly recommended!\",\n",
        "    \"The price is a bit high, but the quality is good.\",\n",
        "    \"I had a terrible experience with this service.\",\n",
        "    \"The new update fixed many bugs, much better now.\",\n",
        "    \"Not satisfied with the performance.\",\n",
        "    \"Excellent product and fast delivery.\"\n",
        "]\n",
        "\n",
        "print(\"--- Initial Data ---\")\n",
        "for i, review in enumerate(customer_reviews):\n",
        "    print(f\"Review {i+1}: {review}\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "\n",
        "# --- Step 2: Text Cleaning ---\n",
        "def clean_text(text):\n",
        "    # Remove special characters and punctuation\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "cleaned_reviews = [clean_text(review) for review in customer_reviews]\n",
        "\n",
        "print(\"--- Cleaned Data ---\")\n",
        "for i, review in enumerate(cleaned_reviews):\n",
        "    print(f\"Review {i+1}: {review}\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "\n",
        "# --- Step 3: Text Preprocessing ---\n",
        "# Download necessary NLTK data (if not already downloaded)\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/stopwords')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "except LookupError:\n",
        "    nltk.download('wordnet')\n",
        "\n",
        "try:\n",
        "    nltk.data.find('sentiment/vader_lexicon')\n",
        "except LookupError:\n",
        "    nltk.download('vader_lexicon')\n",
        "\n",
        "\n",
        "# Initialize lemmatizer and stop words\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stop words and lemmatize\n",
        "    processed_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
        "    return processed_tokens\n",
        "\n",
        "processed_reviews = [preprocess_text(review) for review in cleaned_reviews]\n",
        "\n",
        "print(\"--- Processed Data (Tokens) ---\")\n",
        "for i, tokens in enumerate(processed_reviews):\n",
        "    print(f\"Review {i+1}: {tokens}\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "\n",
        "# --- Step 4: Exploratory Data Analysis (EDA) on Text ---\n",
        "# Join tokens back to strings for EDA and further processing\n",
        "processed_reviews_str = [\" \".join(tokens) for tokens in processed_reviews]\n",
        "\n",
        "# Calculate word frequency\n",
        "all_words = [word for tokens in processed_reviews for word in tokens]\n",
        "freq_dist = nltk.FreqDist(all_words)\n",
        "\n",
        "print(\"--- Most Common Words (EDA) ---\")\n",
        "print(freq_dist.most_common(10))\n",
        "print(\"-\" * 20)\n",
        "\n",
        "\n",
        "# --- Step 5: Sentiment Analysis ---\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "sentiment_scores = [analyzer.polarity_scores(review) for review in processed_reviews_str]\n",
        "\n",
        "print(\"--- Sentiment Scores ---\")\n",
        "for i, scores in enumerate(sentiment_scores):\n",
        "    print(f\"Review {i+1}: {scores}\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "\n",
        "# --- Step 6: Topic Modeling (using NMF as an example) ---\n",
        "# Using TF-IDF for topic modeling\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=1000, max_df=0.95, min_df=2) # Adjust parameters as needed\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(processed_reviews_str)\n",
        "\n",
        "# Apply NMF for topic modeling\n",
        "num_topics = 3 # Define the number of topics\n",
        "nmf_model = NMF(n_components=num_topics, random_state=1)\n",
        "nmf_matrix = nmf_model.fit_transform(tfidf_matrix)\n",
        "\n",
        "# Display top words per topic\n",
        "print(\"--- Topic Modeling (NMF) ---\")\n",
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "for topic_idx, topic in enumerate(nmf_model.components_):\n",
        "    top_words_idx = topic.argsort()[-10:][::-1]\n",
        "    top_words = [feature_names[i] for i in top_words_idx]\n",
        "    print(f\"Topic {topic_idx+1}: {', '.join(top_words)}\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "\n",
        "# --- Step 7: Named Entity Recognition (NER) ---\n",
        "# Load spaCy model (if not already loaded)\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except:\n",
        "    !python -m spacy download en_core_web_sm\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "print(\"--- Named Entity Recognition (NER) ---\")\n",
        "for i, review in enumerate(cleaned_reviews): # Use cleaned reviews for NER\n",
        "    doc = nlp(review)\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    print(f\"Review {i+1} Entities: {entities}\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "\n",
        "# --- Step 8: Feature Engineering (Example: TF-IDF) ---\n",
        "# TF-IDF matrix was already created in Topic Modeling step (tfidf_matrix)\n",
        "# We can use this matrix for downstream tasks like classification.\n",
        "print(\"--- Feature Engineering (TF-IDF Matrix Shape) ---\")\n",
        "print(tfidf_matrix.shape)\n",
        "print(\"-\" * 20)\n",
        "\n",
        "\n",
        "# --- Step 9: Insight Extraction and Reporting ---\n",
        "# This step involves interpreting the results from previous steps.\n",
        "# For example, we would analyze:\n",
        "# - Overall sentiment distribution\n",
        "# - Sentiment trends for specific topics or entities\n",
        "# - Most frequent complaints or praises (from topics and keywords)\n",
        "# - Identify reviews related to specific entities\n",
        "\n",
        "print(\"--- Insight Extraction (Examples) ---\")\n",
        "# Example 1: Reviews with strong negative sentiment\n",
        "negative_reviews_indices = [i for i, scores in enumerate(sentiment_scores) if scores['neg'] > 0.5]\n",
        "print(\"Reviews with strong negative sentiment:\")\n",
        "for i in negative_reviews_indices:\n",
        "    print(f\"- Review {i+1}: {customer_reviews[i]} (Sentiment: {sentiment_scores[i]})\")\n",
        "\n",
        "# Example 2: Reviews related to Topic 1 (based on top words)\n",
        "# We would need to map reviews to topics based on the nmf_matrix\n",
        "# For simplicity, we'll just print the top words of Topic 1 again.\n",
        "print(\"\\nTop words for Topic 1 (potential area of focus):\")\n",
        "print(f\"{', '.join([feature_names[i] for i in nmf_model.components_[0].argsort()[-10:][::-1]])}\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "# --- Step 10: Finish task ---\n",
        "print(\"--- Analysis Complete ---\")\n",
        "print(\"Based on the sentiment scores, topic modeling, and NER, we can now delve deeper into specific areas of customer feedback to extract actionable insights for our fintech startup.\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Initial Data ---\n",
            "Review 1: The product is great! I love the features.\n",
            "Review 2: This app is very slow and crashes frequently.\n",
            "Review 3: Customer service was helpful and resolved my issue quickly.\n",
            "Review 4: The user interface is confusing and hard to navigate.\n",
            "Review 5: Amazing experience, highly recommended!\n",
            "Review 6: The price is a bit high, but the quality is good.\n",
            "Review 7: I had a terrible experience with this service.\n",
            "Review 8: The new update fixed many bugs, much better now.\n",
            "Review 9: Not satisfied with the performance.\n",
            "Review 10: Excellent product and fast delivery.\n",
            "--------------------\n",
            "--- Cleaned Data ---\n",
            "Review 1: the product is great i love the features\n",
            "Review 2: this app is very slow and crashes frequently\n",
            "Review 3: customer service was helpful and resolved my issue quickly\n",
            "Review 4: the user interface is confusing and hard to navigate\n",
            "Review 5: amazing experience highly recommended\n",
            "Review 6: the price is a bit high but the quality is good\n",
            "Review 7: i had a terrible experience with this service\n",
            "Review 8: the new update fixed many bugs much better now\n",
            "Review 9: not satisfied with the performance\n",
            "Review 10: excellent product and fast delivery\n",
            "--------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Processed Data (Tokens) ---\n",
            "Review 1: ['product', 'great', 'love', 'feature']\n",
            "Review 2: ['app', 'slow', 'crash', 'frequently']\n",
            "Review 3: ['customer', 'service', 'helpful', 'resolved', 'issue', 'quickly']\n",
            "Review 4: ['user', 'interface', 'confusing', 'hard', 'navigate']\n",
            "Review 5: ['amazing', 'experience', 'highly', 'recommended']\n",
            "Review 6: ['price', 'bit', 'high', 'quality', 'good']\n",
            "Review 7: ['terrible', 'experience', 'service']\n",
            "Review 8: ['new', 'update', 'fixed', 'many', 'bug', 'much', 'better']\n",
            "Review 9: ['satisfied', 'performance']\n",
            "Review 10: ['excellent', 'product', 'fast', 'delivery']\n",
            "--------------------\n",
            "--- Most Common Words (EDA) ---\n",
            "[('product', 2), ('service', 2), ('experience', 2), ('great', 1), ('love', 1), ('feature', 1), ('app', 1), ('slow', 1), ('crash', 1), ('frequently', 1)]\n",
            "--------------------\n",
            "--- Sentiment Scores ---\n",
            "Review 1: {'neg': 0.0, 'neu': 0.194, 'pos': 0.806, 'compound': 0.8519}\n",
            "Review 2: {'neg': 0.474, 'neu': 0.526, 'pos': 0.0, 'compound': -0.4019}\n",
            "Review 3: {'neg': 0.0, 'neu': 0.471, 'pos': 0.529, 'compound': 0.5423}\n",
            "Review 4: {'neg': 0.524, 'neu': 0.476, 'pos': 0.0, 'compound': -0.3182}\n",
            "Review 5: {'neg': 0.0, 'neu': 0.253, 'pos': 0.747, 'compound': 0.7089}\n",
            "Review 6: {'neg': 0.0, 'neu': 0.58, 'pos': 0.42, 'compound': 0.4404}\n",
            "Review 7: {'neg': 0.608, 'neu': 0.392, 'pos': 0.0, 'compound': -0.4767}\n",
            "Review 8: {'neg': 0.0, 'neu': 0.674, 'pos': 0.326, 'compound': 0.4404}\n",
            "Review 9: {'neg': 0.0, 'neu': 0.263, 'pos': 0.737, 'compound': 0.4215}\n",
            "Review 10: {'neg': 0.0, 'neu': 0.448, 'pos': 0.552, 'compound': 0.5719}\n",
            "--------------------\n",
            "--- Topic Modeling (NMF) ---\n",
            "Topic 1: service, experience, product\n",
            "Topic 2: product, service, experience\n",
            "Topic 3: experience, service, product\n",
            "--------------------\n",
            "--- Named Entity Recognition (NER) ---\n",
            "Review 1 Entities: []\n",
            "Review 2 Entities: []\n",
            "Review 3 Entities: []\n",
            "Review 4 Entities: []\n",
            "Review 5 Entities: []\n",
            "Review 6 Entities: []\n",
            "Review 7 Entities: []\n",
            "Review 8 Entities: []\n",
            "Review 9 Entities: []\n",
            "Review 10 Entities: []\n",
            "--------------------\n",
            "--- Feature Engineering (TF-IDF Matrix Shape) ---\n",
            "(10, 3)\n",
            "--------------------\n",
            "--- Insight Extraction (Examples) ---\n",
            "Reviews with strong negative sentiment:\n",
            "- Review 4: The user interface is confusing and hard to navigate. (Sentiment: {'neg': 0.524, 'neu': 0.476, 'pos': 0.0, 'compound': -0.3182})\n",
            "- Review 7: I had a terrible experience with this service. (Sentiment: {'neg': 0.608, 'neu': 0.392, 'pos': 0.0, 'compound': -0.4767})\n",
            "\n",
            "Top words for Topic 1 (potential area of focus):\n",
            "service, experience, product\n",
            "--------------------\n",
            "--- Analysis Complete ---\n",
            "Based on the sentiment scores, topic modeling, and NER, we can now delve deeper into specific areas of customer feedback to extract actionable insights for our fintech startup.\n"
          ]
        }
      ]
    }
  ]
}