{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **CNN Architecture**"
      ],
      "metadata": {
        "id": "xwcFLL5XifLw"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10ebb298"
      },
      "source": [
        "**1. What is the role of filters and feature maps in Convolutional Neural Network (CNN)?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69dc62b3"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "**Filters (also known as Kernels):**\n",
        "\n",
        "*   **What they are:** Filters are small matrices of numbers that are convolved (slid) across the input image. Each filter is designed to detect a specific feature or pattern in the image, such as edges, corners, textures, or even more complex shapes.\n",
        "*   **Their role:** During the training process, the values within these filters are learned. Different filters learn to respond strongly to different visual features. When a filter is convolved over an image, it produces a high output value when the feature it is designed to detect is present in that region of the image.\n",
        "*   **Think of it like:** Imagine a magnifying glass that we move over an image. Different magnifying glasses (filters) highlight different aspects (features) of the image.\n",
        "\n",
        "**Feature Maps:**\n",
        "\n",
        "*   **What they are:** A feature map is the output of applying a filter to an entire input image. It's essentially a representation of the input image where the values indicate the presence and strength of the feature that the filter was designed to detect at each location.\n",
        "*   **Their role:** Each filter applied to the input image generates a different feature map. These feature maps capture different aspects of the image, providing a rich representation of the input data. Subsequent layers in the CNN then take these feature maps as input and learn to detect more complex patterns by combining the features from the previous layers.\n",
        "*   **Think of it like:** If we have a filter that detects horizontal edges, the resulting feature map will show us where all the horizontal edges are located in the original image. If we have another filter that detects vertical edges, its feature map will show the locations of vertical edges.\n",
        "\n",
        "**In summary:**\n",
        "\n",
        "Filters are the tools that scan the image for specific features, and feature maps are the results of that scan, highlighting where those features are present in the image. By using multiple filters in each layer, a CNN can learn to extract a wide range of features from the input data, which is essential for tasks like image classification, object detection, and image generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7d63e9a"
      },
      "source": [
        "**2. Explain the concepts of padding and stride in CNNs (Convolutional Neural Network). How do they affect the output dimensions of feature maps?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9104e97d"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "**Padding:**\n",
        "\n",
        "*   **What it is:** Padding involves adding extra pixels (typically zeros) around the border of the input image or feature map before convolution.\n",
        "*   **Why it's used:**\n",
        "    *   **Preserving spatial dimensions:** Without padding, the spatial dimensions (height and width) of the output feature map decrease with each convolutional layer. Padding helps to maintain the spatial size, preventing the loss of information, especially at the edges of the input.\n",
        "    *   **Using information at the edges:** Pixels at the edges of an image are only involved in the convolution process a few times, compared to pixels in the center. Padding ensures that edge pixels are also considered sufficiently, allowing the network to learn features from the entire image.\n",
        "*   **Types of padding:** Common types include \"valid\" (no padding) and \"same\" (padding is added so that the output dimensions are the same as the input dimensions, given a stride of 1).\n",
        "\n",
        "**Stride:**\n",
        "\n",
        "*   **What it is:** Stride is the number of pixels the filter shifts over the input image or feature map at each step of the convolution.\n",
        "*   **Why it's used:**\n",
        "    *   **Reducing spatial dimensions:** A stride greater than 1 reduces the spatial dimensions of the output feature map. This can help to reduce the computational cost of the network and downsample the feature maps.\n",
        "    *   **Controlling the receptive field:** Stride affects how much of the input the filter \"sees\" at each step. A larger stride means the filter covers a larger area with fewer steps.\n",
        "\n",
        "**How they affect the output dimensions of feature maps:**\n",
        "\n",
        "The output dimensions of a convolutional layer are determined by the input dimensions, the filter size, the padding, and the stride. The formula for calculating the output dimension (for one dimension, say height or width) is:\n",
        "\n",
        "`Output Dimension = [(Input Dimension - Filter Size + 2 * Padding) / Stride] + 1`\n",
        "\n",
        "*   **Padding:** Increases the input dimension effectively, leading to a larger output dimension. With \"same\" padding and a stride of 1, the output dimension is the same as the input dimension.\n",
        "*   **Stride:** A larger stride reduces the number of steps the filter takes, resulting in a smaller output dimension.\n",
        "\n",
        "In essence, padding helps to preserve spatial information and ensures edge features are considered, while stride helps to reduce spatial dimensions and computational complexity. Both are important hyperparameters that need to be carefully chosen when designing a CNN architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cf3afb4"
      },
      "source": [
        "**3. Define receptive field in the context of CNNs. Why is it important for deep architectures?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3aeafeb8"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "**Receptive Field:**\n",
        "\n",
        "*   **What it is:** The receptive field of a neuron in a CNN is the region in the input image (or the previous layer's feature map) that influences that neuron's activation. In other words, it's the area of the input that a particular filter is \"seeing\" when it computes its output at a specific location in the feature map.\n",
        "*   **How it grows:** As we go deeper into a CNN (from earlier layers to later layers), the receptive field of the neurons in those layers increases. This is because each neuron in a deeper layer is influenced by a larger area of the previous layer's feature map, which in turn corresponds to an even larger area of the original input image. The receptive field grows due to the combined effect of convolution, pooling, and stride operations.\n",
        "\n",
        "**Why it's important for deep architectures:**\n",
        "\n",
        "*   **Capturing hierarchical features:** The increasing receptive field size in deeper layers allows the network to capture increasingly complex and abstract features. Early layers with small receptive fields detect simple features like edges and corners. Deeper layers with larger receptive fields can combine these simple features to detect more complex patterns like shapes, textures, and eventually, objects.\n",
        "*   **Understanding global context:** As the receptive field expands, neurons in deeper layers are able to incorporate information from a wider region of the input. This is crucial for understanding the global context of an image and making decisions based on the relationships between different parts of the image. For tasks like image classification, a large receptive field is necessary to consider the entire image when determining its class.\n",
        "*   **Efficiency:** Deep architectures with increasing receptive fields can process information efficiently. Instead of having a single large filter that covers the entire image in the first layer (which would be computationally expensive), the network uses multiple layers with smaller filters and increasing receptive fields. This allows the network to learn hierarchical representations and capture complex patterns with fewer parameters.\n",
        "\n",
        "In summary, the receptive field is a key concept in CNNs that explains how neurons in deeper layers can see and process information from larger areas of the input. Its growth in deep architectures is essential for capturing hierarchical features, understanding global context, and achieving computational efficiency."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb3fa5f2"
      },
      "source": [
        "**4. Discuss how filter size and stride influence the number of parameters in a CNN.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79637da1"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "The number of parameters in a convolutional layer is primarily determined by the filter size and the number of filters. Stride does **not** directly affect the number of parameters within a single convolutional layer. Here's the breakdown:\n",
        "\n",
        "**Filter Size:**\n",
        "\n",
        "*   **Direct Influence:** The filter size has a direct and significant impact on the number of parameters. Each filter is a small matrix of weights that the network learns. The number of parameters in a single filter is simply its dimensions (height \\* width) multiplied by the number of input channels.\n",
        "*   **Example:** If we have an input image with 3 channels (like RGB) and we use a 3x3 filter, that filter has 3 \\* 3 \\* 3 = 27 parameters. If we increase the filter size to 5x5, that filter now has 5 \\* 5 \\* 3 = 75 parameters.\n",
        "*   **Total Parameters:** The total number of parameters in a convolutional layer is the number of parameters per filter multiplied by the number of filters in that layer. So, increasing the filter size or the number of filters will increase the total number of parameters.\n",
        "\n",
        "**Stride:**\n",
        "\n",
        "*   **Indirect Influence:** Stride does **not** change the number of weights within each filter. The filter itself remains the same size regardless of the stride.\n",
        "*   **Effect on Output Size:** Stride affects the size of the output feature map. A larger stride results in a smaller output feature map. While this doesn't change the number of parameters in the convolutional layer itself, it can affect the number of parameters in subsequent layers (like fully connected layers) if their input size depends on the output size of the convolutional layer. However, for the convolutional layer specifically, stride does not influence the number of parameters.\n",
        "\n",
        "**In summary:**\n",
        "\n",
        "*   **Filter size** directly impacts the number of parameters in a convolutional layer: larger filters mean more parameters per filter, and more filters mean more total parameters.\n",
        "*   **Stride** does **not** directly affect the number of parameters in a convolutional layer. Its main influence is on the spatial dimensions of the output feature map."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef16d1d3"
      },
      "source": [
        "**5. Compare and contrast different CNN-based architectures like LeNet, AlexNet, and VGG in terms of depth, filter sizes, and performance.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "703a3707"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "**LeNet:**\n",
        "\n",
        "*   **Depth:** One of the earliest CNNs, relatively shallow with a few convolutional and pooling layers followed by fully connected layers.\n",
        "*   **Filter Sizes:** Used smaller filter sizes (e.g., 5x5).\n",
        "*   **Performance:** Primarily used for digit recognition tasks. Achieved good performance for its time on datasets like MNIST. Limited in performance on more complex image recognition tasks due to its shallow depth and limited capacity.\n",
        "\n",
        "**AlexNet:**\n",
        "\n",
        "*   **Depth:** Significantly deeper than LeNet, with more convolutional and pooling layers. It was one of the first deep CNNs to achieve breakthrough performance on a large-scale image dataset.\n",
        "*   **Filter Sizes:** Used a mix of filter sizes, including larger ones in the initial layers (e.g., 11x11) and smaller ones in later layers (e.g., 3x3).\n",
        "*   **Performance:** Achieved a significant improvement in performance on the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012, demonstrating the power of deep CNNs for complex image recognition tasks. Introduced the use of ReLU activation functions and dropout for regularization.\n",
        "\n",
        "**VGG:**\n",
        "\n",
        "*   **Depth:** Even deeper than AlexNet, exploring the idea that increasing depth by using many small filters is beneficial. VGG networks come in different versions with varying depths (e.g., VGG16, VGG19).\n",
        "*   **Filter Sizes:** Primarily used small 3x3 convolutional filters throughout the network. This allowed for increasing the receptive field with depth while keeping the number of parameters manageable.\n",
        "*   **Performance:** Showed that increasing depth with small filters leads to improved performance on large-scale image recognition tasks. VGG networks are known for their simplicity and effectiveness, although they can be computationally expensive due to their depth.\n",
        "\n",
        "**Comparison and Contrast:**\n",
        "\n",
        "*   **Depth:** LeNet is the shallowest, followed by AlexNet, and then VGG which is the deepest among the three. Increasing depth generally led to improved performance on more complex datasets.\n",
        "*   **Filter Sizes:** LeNet used primarily 5x5 filters. AlexNet used a mix of larger and smaller filters. VGG consistently used small 3x3 filters. The use of smaller filters in deeper networks allowed for a larger receptive field with fewer parameters compared to using larger filters.\n",
        "*   **Performance:** Each architecture represented a step forward in CNN performance, with AlexNet significantly outperforming LeNet on large-scale datasets, and VGG further improving performance by exploring increased depth with small filters.\n",
        "\n",
        "In summary, these architectures demonstrate the evolution of CNN design, highlighting the importance of depth and the strategic use of filter sizes for achieving high performance on image recognition tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6. Using keras, build and train a simple CNN model on the MNIST dataset from scratch. Include code for module creation, compilation, training, and evaluation.**"
      ],
      "metadata": {
        "id": "gZPhcSyl7Iiw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 755
        },
        "id": "4f6cfe3f",
        "outputId": "745d4f45-8283-4bdf-b459-1d978cd45b6e"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# 1. Load and preprocess the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess the images\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32') / 255\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32') / 255\n",
        "\n",
        "# Preprocess the labels\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# 2. Build the CNN model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# 3. Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n",
        "\n",
        "# 4. Train the model\n",
        "print(\"\\nTraining the model...\")\n",
        "history = model.fit(x_train, y_train, epochs=5, batch_size=200, verbose=2, validation_data=(x_test, y_test))\n",
        "\n",
        "# 5. Evaluate the model\n",
        "print(\"\\nEvaluating the model...\")\n",
        "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "# 6. Finish task\n",
        "print(f'\\nModel Evaluation Results:')\n",
        "print(f'Test Loss: {loss:.4f}')\n",
        "print(f'Test Accuracy: {accuracy:.4f}')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m204,928\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">204,928</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m225,034\u001b[0m (879.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">225,034</span> (879.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m225,034\u001b[0m (879.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">225,034</span> (879.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training the model...\n",
            "Epoch 1/5\n",
            "300/300 - 6s - 18ms/step - accuracy: 0.9271 - loss: 0.2495 - val_accuracy: 0.9765 - val_loss: 0.0753\n",
            "Epoch 2/5\n",
            "300/300 - 2s - 6ms/step - accuracy: 0.9801 - loss: 0.0649 - val_accuracy: 0.9805 - val_loss: 0.0576\n",
            "Epoch 3/5\n",
            "300/300 - 2s - 5ms/step - accuracy: 0.9856 - loss: 0.0466 - val_accuracy: 0.9843 - val_loss: 0.0484\n",
            "Epoch 4/5\n",
            "300/300 - 2s - 5ms/step - accuracy: 0.9890 - loss: 0.0357 - val_accuracy: 0.9896 - val_loss: 0.0339\n",
            "Epoch 5/5\n",
            "300/300 - 2s - 5ms/step - accuracy: 0.9911 - loss: 0.0281 - val_accuracy: 0.9865 - val_loss: 0.0402\n",
            "\n",
            "Evaluating the model...\n",
            "\n",
            "Model Evaluation Results:\n",
            "Test Loss: 0.0402\n",
            "Test Accuracy: 0.9865\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. Load and preprocess the CIFAR-10 dataset using Keras, and create a CNN model to classify RGB images. Show your preprocessing and architecture.**"
      ],
      "metadata": {
        "id": "thuCo37I7iWF"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "f198adbd",
        "outputId": "b0f473dd-7c0d-4bcb-ab63-64ab3ecfb58c"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# 1. Load and preprocess the CIFAR-10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train = to_categorical(y_train, num_classes=10)\n",
        "y_test = to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# 2. Build the CNN model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "# 3. Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n",
        "\n",
        "# 4. Train the model\n",
        "print(\"\\nTraining the model...\")\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=32, verbose=2, validation_data=(x_test, y_test))\n",
        "\n",
        "# 5. Evaluate the model\n",
        "print(\"\\nEvaluating the model...\")\n",
        "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "# 6. Finish task\n",
        "print(f'\\nModel Evaluation Results:')\n",
        "print(f'Test Loss: {loss:.4f}')\n",
        "print(f'Test Accuracy: {accuracy:.4f}')\n",
        "\n",
        "print(\"\\nPreprocessing steps:\")\n",
        "print(\"- Normalized pixel values to be between 0 and 1\")\n",
        "print(\"- One-hot encoded the labels\")\n",
        "\n",
        "print(\"\\nCNN Architecture:\")\n",
        "model.summary()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m36,928\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m65,600\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,600</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m122,570\u001b[0m (478.79 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">122,570</span> (478.79 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m122,570\u001b[0m (478.79 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">122,570</span> (478.79 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training the model...\n",
            "Epoch 1/10\n",
            "1563/1563 - 13s - 8ms/step - accuracy: 0.4498 - loss: 1.5080 - val_accuracy: 0.5285 - val_loss: 1.3116\n",
            "Epoch 2/10\n",
            "1563/1563 - 6s - 4ms/step - accuracy: 0.5931 - loss: 1.1484 - val_accuracy: 0.6090 - val_loss: 1.0954\n",
            "Epoch 3/10\n",
            "1563/1563 - 5s - 3ms/step - accuracy: 0.6506 - loss: 0.9950 - val_accuracy: 0.6569 - val_loss: 0.9827\n",
            "Epoch 4/10\n",
            "1563/1563 - 6s - 4ms/step - accuracy: 0.6865 - loss: 0.8930 - val_accuracy: 0.6749 - val_loss: 0.9351\n",
            "Epoch 5/10\n",
            "1563/1563 - 5s - 3ms/step - accuracy: 0.7157 - loss: 0.8190 - val_accuracy: 0.6822 - val_loss: 0.9244\n",
            "Epoch 6/10\n",
            "1563/1563 - 6s - 4ms/step - accuracy: 0.7345 - loss: 0.7589 - val_accuracy: 0.7100 - val_loss: 0.8460\n",
            "Epoch 7/10\n",
            "1563/1563 - 5s - 3ms/step - accuracy: 0.7500 - loss: 0.7123 - val_accuracy: 0.6913 - val_loss: 0.9335\n",
            "Epoch 8/10\n",
            "1563/1563 - 6s - 4ms/step - accuracy: 0.7655 - loss: 0.6702 - val_accuracy: 0.7012 - val_loss: 0.8794\n",
            "Epoch 9/10\n",
            "1563/1563 - 5s - 3ms/step - accuracy: 0.7800 - loss: 0.6265 - val_accuracy: 0.7142 - val_loss: 0.8365\n",
            "Epoch 10/10\n",
            "1563/1563 - 6s - 4ms/step - accuracy: 0.7943 - loss: 0.5867 - val_accuracy: 0.7132 - val_loss: 0.8832\n",
            "\n",
            "Evaluating the model...\n",
            "\n",
            "Model Evaluation Results:\n",
            "Test Loss: 0.8832\n",
            "Test Accuracy: 0.7132\n",
            "\n",
            "Preprocessing steps:\n",
            "- Normalized pixel values to be between 0 and 1\n",
            "- One-hot encoded the labels\n",
            "\n",
            "CNN Architecture:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │        \u001b[38;5;34m36,928\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m65,600\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │           \u001b[38;5;34m650\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,600</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m367,712\u001b[0m (1.40 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">367,712</span> (1.40 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m122,570\u001b[0m (478.79 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">122,570</span> (478.79 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m245,142\u001b[0m (957.59 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">245,142</span> (957.59 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8. Using PyTorch, write a script to define and train a CNN on the MNIST dataset. Include model definition, data loaders, training loop, and accuracy evaluation.**"
      ],
      "metadata": {
        "id": "ZGlD5-ie8kv6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "049aea6a",
        "outputId": "67f9cb1b-5c14-49f8-e432-255873f337e9"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 1. Import necessary libraries (already done above)\n",
        "\n",
        "# 2. Define the CNN model\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 7 * 7) # Flatten the output for the dense layer\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# 3. Load and prepare the MNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# 4. Define loss function and optimizer\n",
        "model = SimpleCNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 5. Implement the training loop\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
        "\n",
        "# 6. Evaluate the model\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)\\n')\n",
        "\n",
        "# 7. Train and evaluate the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "epochs = 5\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(model, device, train_loader, optimizer, epoch)\n",
        "    test(model, device, test_loader)\n",
        "\n",
        "# 8. Finish task (results are printed in the test function)\n",
        "print(\"\\nPyTorch CNN training and evaluation on MNIST complete.\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 18.0MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 484kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.85MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 11.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.298279\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.230452\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.238434\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.136265\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.008662\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.186962\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.022989\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.137028\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.094320\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.003838\n",
            "\n",
            "Test set: Average loss: 0.0001, Accuracy: 9832/10000 (98%)\n",
            "\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.027377\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.066142\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.062755\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.004218\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.014254\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.005315\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.075454\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.021928\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.078252\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.011818\n",
            "\n",
            "Test set: Average loss: 0.0000, Accuracy: 9894/10000 (99%)\n",
            "\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.007918\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.010694\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.210354\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.051461\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.002386\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.008861\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.004894\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.047899\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.036278\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.020815\n",
            "\n",
            "Test set: Average loss: 0.0000, Accuracy: 9910/10000 (99%)\n",
            "\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.074975\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.007455\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.014095\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.024423\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.036456\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.013485\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.018309\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.022852\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.041167\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.003765\n",
            "\n",
            "Test set: Average loss: 0.0000, Accuracy: 9906/10000 (99%)\n",
            "\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.001069\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.004782\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.005890\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.015840\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.000874\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.005875\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.016664\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.183209\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.001389\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.029486\n",
            "\n",
            "Test set: Average loss: 0.0000, Accuracy: 9895/10000 (99%)\n",
            "\n",
            "\n",
            "PyTorch CNN training and evaluation on MNIST complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**9. Given a custom image dataset stored in a local directory, write code using Keras ImageDataGenerator to preprocess and train a CNN model.**"
      ],
      "metadata": {
        "id": "oZdhaQ_yDzh3"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9cb0784"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "'''\n",
        "Define the path to the dataset\n",
        "\n",
        "Note: Before running this code, make sure the dataset is organized in the following structure:\n",
        "\n",
        "your_dataset_directory/\n",
        "  train/\n",
        "    class1/\n",
        "      img1.jpg\n",
        "      img2.jpg\n",
        "      ...\n",
        "    class2/\n",
        "      imgA.jpg\n",
        "      imgB.jpg\n",
        "      ...\n",
        "    ...\n",
        "  validation/\n",
        "    class1/\n",
        "      img3.jpg\n",
        "      img4.jpg\n",
        "      ...\n",
        "    class2/\n",
        "      imgC.jpg\n",
        "      imgD.jpg\n",
        "      ...\n",
        "    ...\n",
        "\n",
        "'''\n",
        "dataset_dir = \"path/to/your/dataset\"\n",
        "\n",
        "# Define image dimensions and batch size\n",
        "img_height = 128\n",
        "img_width = 128\n",
        "batch_size = 32\n",
        "\n",
        "# 3. Instantiate ImageDataGenerator\n",
        "# We can add data augmentation here (e.g., rotation_range, width_shift_range, etc.)\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2) # Using 20% for validation\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# 4. Prepare data generators\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    dataset_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training') # Specify training subset\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    dataset_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation') # Specify validation subset\n",
        "\n",
        "# Get the number of classes\n",
        "num_classes = train_generator.num_classes\n",
        "\n",
        "# 5. Build the CNN model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# 6. Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n",
        "\n",
        "# 7. Train the model\n",
        "print(\"\\nTraining the model...\")\n",
        "# Using steps_per_epoch and validation_steps with generators is recommended\n",
        "steps_per_epoch = train_generator.samples // batch_size\n",
        "validation_steps = validation_generator.samples // batch_size\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=10,  # We can adjust the number of epochs\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_steps)\n",
        "\n",
        "# 8. Evaluate the model (optional, as validation during training gives performance)\n",
        "# loss, accuracy = model.evaluate(validation_generator, steps=validation_steps)\n",
        "# print(f'\\nModel Evaluation Results:')\n",
        "# print(f'Validation Loss: {loss:.4f}')\n",
        "# print(f'Validation Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# 9. Finish task - The training history contains the performance metrics\n",
        "print(\"\\nCNN training on custom dataset with ImageDataGenerator complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**10. You are working on a web application for a medical imaging startup. Your task is to build and deploy a CNN model that classifies chest X-ray images into “Normal” and “Pneumonia” categories. Describe your end-to-end approach–from data preparation and model training to deploying the model as a web app using Streamlit.**"
      ],
      "metadata": {
        "id": "px2pc_P4FYiq"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "140dc4e5"
      },
      "source": [
        "### Subtask:\n",
        "Create dummy directories and files to simulate the chest X-ray dataset structure.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c95505d1"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires creating a dummy dataset structure for a chest X-ray classification problem.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48d81e72",
        "outputId": "3bcb83ac-e847-4d55-fa2c-3e72884d1a51"
      },
      "source": [
        "from PIL import Image\n",
        "\n",
        "# 2. Define the base directory for the simulated dataset.\n",
        "dataset_dir = \"chest_xray_dataset\"\n",
        "\n",
        "# Define the number of dummy images to create per class per set\n",
        "num_dummy_images = 50\n",
        "\n",
        "# Define the classes\n",
        "classes = [\"Normal\", \"Pneumonia\"]\n",
        "\n",
        "# Define the sets\n",
        "sets = [\"train\", \"validation\"]\n",
        "\n",
        "# Define dummy image properties\n",
        "img_height_dummy = 128\n",
        "img_width_dummy = 128\n",
        "\n",
        "# Remove the existing dummy directories and files to start fresh\n",
        "import shutil\n",
        "if os.path.exists(dataset_dir):\n",
        "    shutil.rmtree(dataset_dir)\n",
        "    print(f\"Removed existing directory: {dataset_dir}\")\n",
        "\n",
        "# 3. Create the 'train' and 'validation' directories within the base directory.\n",
        "# 4. Create 'Normal' and 'Pneumonia' subdirectories within both 'train' and 'validation' directories.\n",
        "for data_set in sets:\n",
        "    for class_name in classes:\n",
        "        # Create the full path for the subdirectory\n",
        "        subdir_path = os.path.join(dataset_dir, data_set, class_name)\n",
        "\n",
        "        # Create the subdirectory if it doesn't exist\n",
        "        os.makedirs(subdir_path, exist_ok=True)\n",
        "        print(f\"Created directory: {subdir_path}\")\n",
        "\n",
        "        # 5. For each class ('Normal' and 'Pneumonia') and each set ('train' and 'validation'), create a loop to generate a specified number of dummy image files\n",
        "        for i in range(num_dummy_images):\n",
        "            # 6. Inside the loop, create a dummy file path using the class name and a sequential index\n",
        "            dummy_file_name = f\"{class_name}_{i:03d}.jpg\"\n",
        "            dummy_file_path = os.path.join(subdir_path, dummy_file_name)\n",
        "\n",
        "            # 7. Create a dummy image and save it\n",
        "            try:\n",
        "                # Create a blank white image\n",
        "                img = Image.new('RGB', (img_width_dummy, img_height_dummy), color = 'white')\n",
        "                img.save(dummy_file_path)\n",
        "                # 8. Print messages indicating the creation of directories and dummy files for clarity.\n",
        "                # print(f\"Created dummy image: {dummy_file_path}\") # Optional: uncomment to see each file created\n",
        "            except IOError as e:\n",
        "                print(f\"Error creating dummy image {dummy_file_path}: {e}\")\n",
        "\n",
        "print(\"\\nDummy image dataset structure created successfully.\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed existing directory: chest_xray_dataset\n",
            "Created directory: chest_xray_dataset/train/Normal\n",
            "Created directory: chest_xray_dataset/train/Pneumonia\n",
            "Created directory: chest_xray_dataset/validation/Normal\n",
            "Created directory: chest_xray_dataset/validation/Pneumonia\n",
            "\n",
            "Dummy image dataset structure created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8121f3de"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous step successfully created the dummy dataset structure. The next step is to load and preprocess this dummy data using Keras ImageDataGenerator. This prepares the data for model training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a897baec",
        "outputId": "2aa2d5c1-4327-48fa-8edb-6f688507d477"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Define image dimensions and batch size\n",
        "img_height = 128\n",
        "img_width = 128\n",
        "batch_size = 32\n",
        "\n",
        "# Instantiate ImageDataGenerator\n",
        "# We can add data augmentation here (e.g., rotation_range, width_shift_range, etc.)\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2) # Using 20% for validation\n",
        "\n",
        "# Prepare data generators\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    dataset_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training') # Specify training subset\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    dataset_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation') # Specify validation subset\n",
        "\n",
        "# Get the number of classes\n",
        "num_classes = train_generator.num_classes\n",
        "\n",
        "print(\"\\nData generators created using ImageDataGenerator.\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 160 images belonging to 2 classes.\n",
            "Found 40 images belonging to 2 classes.\n",
            "\n",
            "Data generators created using ImageDataGenerator.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7f23d3f"
      },
      "source": [
        "**Reasoning**:\n",
        "The data generators have been successfully created. The next logical step is to define and compile the CNN model architecture for the chest X-ray classification task using Keras, as this is necessary before training.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "4be9bad7",
        "outputId": "5ba8a067-9d62-4745-dd2a-e888bbe8ab9c"
      },
      "source": [
        "# Build the CNN model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(img_height, img_width, 3)))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n",
        "\n",
        "print(\"\\nCNN model defined and compiled.\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_4\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_4\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_11 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m126\u001b[0m, \u001b[38;5;34m32\u001b[0m)   │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_10 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m63\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_12 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m61\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_11 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_13 (\u001b[38;5;33mConv2D\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m28\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │        \u001b[38;5;34m73,856\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_12 (\u001b[38;5;33mMaxPooling2D\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_4 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25088\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │     \u001b[38;5;34m3,211,392\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m258\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">126</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">61</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">28</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25088</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,211,392</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">258</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,304,898\u001b[0m (12.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,304,898</span> (12.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m3,304,898\u001b[0m (12.61 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,304,898</span> (12.61 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CNN model defined and compiled.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "907c0005"
      },
      "source": [
        "**Reasoning**:\n",
        "The model is defined and compiled, and the data generators are ready. The next step is to train the CNN model using the prepared data generators.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHvgRiQ_Jngu",
        "outputId": "8aa192e1-dd94-4500-fd88-ee740642f9e9"
      },
      "source": [
        "# Train the model\n",
        "print(\"\\nTraining the model...\")\n",
        "# Using steps_per_epoch and validation_steps with generators is recommended\n",
        "steps_per_epoch = train_generator.samples // batch_size\n",
        "validation_steps = validation_generator.samples // batch_size\n",
        "\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    epochs=5,  # Reduced epochs for demonstration with dummy data\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=validation_steps)\n",
        "\n",
        "print(\"\\nCNN model training complete.\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training the model...\n",
            "Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 170ms/step - accuracy: 0.4468 - loss: 1.2342 - val_accuracy: 0.5312 - val_loss: 0.6951\n",
            "Epoch 2/5\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5108 - loss: 0.7050 - val_accuracy: 0.4062 - val_loss: 0.7067\n",
            "Epoch 3/5\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.5460 - loss: 0.7078 - val_accuracy: 0.5312 - val_loss: 0.6937\n",
            "Epoch 4/5\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.4965 - loss: 0.7089 - val_accuracy: 0.5000 - val_loss: 0.6967\n",
            "Epoch 5/5\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.5417 - loss: 0.6885 - val_accuracy: 0.5000 - val_loss: 0.6932\n",
            "\n",
            "CNN model training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dada9bb9"
      },
      "source": [
        "**Reasoning**:\n",
        "The model has been trained on the dummy data. The next step is to save the trained model so it can be used for deployment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HYh2kX4QJ03J",
        "outputId": "e1087b62-e2cc-49f2-ccc5-a8f45676050a"
      },
      "source": [
        "# Save the trained model\n",
        "model_save_path = \"chest_xray_cnn_model.keras\"\n",
        "model.save(model_save_path)\n",
        "\n",
        "print(f\"\\nCNN model saved to: {model_save_path}\")\n",
        "\n",
        "# 9. Finish task - The model is trained and saved."
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "CNN model saved to: chest_xray_cnn_model.keras\n"
          ]
        }
      ]
    }
  ]
}