{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Useful NLP Libraries & Networks**"
      ],
      "metadata": {
        "id": "iUfnjYzL9jgE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Compare and contrast NLTK and spaCy in terms of features, ease of use, and performance.**"
      ],
      "metadata": {
        "id": "bhwAe6QTcoMS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08bbd998"
      },
      "source": [
        "Answer:\n",
        "\n",
        "**NLTK (Natural Language Toolkit)**\n",
        "\n",
        "*   **Features:** NLTK is a comprehensive library for various NLP tasks, including tokenization, stemming, tagging, parsing, and semantic reasoning. It is often used for research and teaching due to its wide range of algorithms and corpora.\n",
        "*   **Ease of Use:** NLTK can have a steeper learning curve, especially for beginners, as it requires understanding various modules and concepts. It's more focused on providing building blocks for NLP tasks rather than offering ready-to-use pipelines.\n",
        "*   **Performance:** NLTK is generally slower compared to spaCy, particularly for common tasks like tokenization and POS tagging, as it is not designed for production-level speed.\n",
        "\n",
        "**spaCy**\n",
        "\n",
        "*   **Features:** spaCy is designed for efficiency and production use. It provides highly optimized and fast implementations for core NLP tasks like tokenization, POS tagging, named entity recognition (NER), and dependency parsing. It also includes pre-trained models for various languages.\n",
        "*   **Ease of Use:** spaCy is known for its user-friendly API and streamlined workflow. It provides pre-built pipelines that make it easier to perform common NLP tasks with less code.\n",
        "*   **Performance:** spaCy is significantly faster than NLTK for most tasks, making it a preferred choice for applications requiring high throughput and real-time processing.\n",
        "\n",
        "**Comparison Summary:**\n",
        "\n",
        "| Feature        | NLTK                                    | spaCy                                     |\n",
        "| :------------- | :-------------------------------------- | :---------------------------------------- |\n",
        "| **Focus**      | Research, teaching, broad functionality | Production, efficiency, ready-to-use      |\n",
        "| **Features**   | Comprehensive, many algorithms & corpora | Optimized core NLP tasks, pre-trained models |\n",
        "| **Ease of Use**| Steeper learning curve, building blocks | User-friendly API, pre-built pipelines    |\n",
        "| **Performance**| Generally slower                        | Significantly faster                      |\n",
        "\n",
        "In conclusion, NLTK is excellent for exploring various NLP concepts and algorithms, especially in academic settings. spaCy is better suited for building efficient and scalable NLP applications due to its speed and ease of use. The choice between the two depends on the specific needs of our project."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **What is TextBlob and how does it simplify common NLP tasks like sentiment analysis and translation?**"
      ],
      "metadata": {
        "id": "UfiQ18K-GFMF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bcce55b"
      },
      "source": [
        "Answer:\n",
        "\n",
        "**TextBlob**\n",
        "\n",
        "TextBlob is a Python library built on top of NLTK and Pattern that aims to simplify common NLP tasks with a clean and accessible API. It provides a simple interface to dive into common tasks like:\n",
        "\n",
        "* **Sentiment Analysis:** TextBlob offers built-in sentiment analysis capabilities. We can easily get the polarity and subjectivity of a text. Polarity is a float within the range [-1.0, 1.0] where -1.0 is negative and 1.0 is positive. Subjectivity is a float within the range [0.0, 1.0] where 0.0 is very objective and 1.0 is very subjective.\n",
        "* **Translation and Language Detection:** TextBlob can also detect the language of a text and translate it to another language using Google Translate. This is done through a simple method call, abstracting away the complexities of interacting with translation APIs.\n",
        "\n",
        "**How it simplifies tasks:**\n",
        "\n",
        "TextBlob simplifies these tasks by providing a user-friendly, object-oriented interface. Instead of dealing with separate functions or modules for each task (as we might in NLTK), we create a `TextBlob` object and access its properties or methods to perform operations like sentiment analysis (`.sentiment`) or translation (`.translate()`). This makes the code more readable and intuitive, especially for those new to NLP."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **Explain the role of Standford NLP in academic and industry NLP Projects.**"
      ],
      "metadata": {
        "id": "JDM8BtTdI5nY"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46046a6e"
      },
      "source": [
        "Answer:\n",
        "\n",
        "**Stanford NLP (Stanford CoreNLP)**\n",
        "\n",
        "Stanford NLP, particularly the Stanford CoreNLP suite, is a widely-used collection of NLP tools and libraries developed by the Stanford Natural Language Processing Group. It's a Java-based library that provides a broad range of linguistic analysis tools, making it valuable in both academic and industry settings.\n",
        "\n",
        "**Role in Academic Projects:**\n",
        "\n",
        "* **Comprehensive Linguistic Analysis:** Stanford CoreNLP offers a deep level of linguistic analysis, including tokenization, part-of-speech tagging, named entity recognition, sentiment analysis, coreference resolution, and dependency parsing. This makes it a powerful tool for researchers studying various linguistic phenomena and developing new NLP models.\n",
        "* **Benchmarking and Comparison:** Due to its widespread use and comprehensive features, Stanford CoreNLP is often used as a baseline or benchmark for comparing the performance of new NLP algorithms and models in academic research.\n",
        "* **Resource for Research:** The Stanford NLP group also releases various datasets, corpora, and pre-trained models that are valuable resources for academic research in NLP.\n",
        "\n",
        "**Role in Industry Projects:**\n",
        "\n",
        "* **Robust and Mature Tools:** Stanford CoreNLP is a mature and well-maintained library with a long history of development. This makes it a reliable choice for industry applications that require robust and stable NLP tools.\n",
        "* **Integration with Various Applications:** While Java-based, Stanford CoreNLP can be integrated into various applications and workflows, often through wrappers or APIs in other languages like Python. This allows companies to leverage its powerful analysis capabilities within their existing systems.\n",
        "* **Specific Task Performance:** For certain tasks like coreference resolution and dependency parsing, Stanford CoreNLP is known for providing high-quality results, making it a preferred choice for applications that heavily rely on these specific analyses (e.g., information extraction, question answering).\n",
        "* **Commercial Support:** Stanford NLP also offers commercial licenses and support, which can be beneficial for companies using the tools in production environments.\n",
        "\n",
        "In summary, Stanford NLP plays a significant role in both academic and industry NLP projects by providing comprehensive and robust linguistic analysis tools, serving as a benchmark for research, and offering valuable resources for researchers and practitioners. Its strengths lie in deep linguistic analysis and mature, reliable tools for various NLP tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **Describe the architecture and functioning of a Recurrent Natural Network (RNN).**"
      ],
      "metadata": {
        "id": "A_K_gg_8lnxR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88616718"
      },
      "source": [
        "Answer:\n",
        "\n",
        "**Recurrent Neural Network (RNN)**\n",
        "\n",
        "A Recurrent Neural Network (RNN) is a type of artificial neural network designed to process sequential data, such as time series, text, and speech. Unlike traditional feedforward neural networks where information flows in only one direction, RNNs have connections that allow information to flow in a loop, enabling them to maintain an internal memory of previous inputs.\n",
        "\n",
        "**Architecture:**\n",
        "\n",
        "The core idea behind an RNN is the presence of a hidden state (or memory) that is updated at each time step based on the current input and the hidden state from the previous time step. This hidden state captures information about the sequence processed so far.\n",
        "\n",
        "A simple RNN cell typically consists of:\n",
        "\n",
        "*   **Input Layer:** Receives the input at the current time step.\n",
        "*   **Hidden Layer:** Contains recurrent connections that allow information to persist across time steps. The activation of neurons in the hidden layer at time `t` is a function of the current input at time `t` and the hidden state at time `t-1`.\n",
        "*   **Output Layer:** Produces the output at the current time step based on the hidden state.\n",
        "\n",
        "The recurrent connection is the key feature. It allows the network to pass information from one time step to the next, effectively giving it a memory.\n",
        "\n",
        "**Functioning:**\n",
        "\n",
        "The functioning of an RNN can be described as follows:\n",
        "\n",
        "1.  At each time step `t`, the RNN takes the current input `x_t` and the hidden state from the previous time step `h_{t-1}`.\n",
        "2.  These inputs are processed through the hidden layer, typically using a non-linear activation function (like tanh or ReLU), to compute the new hidden state `h_t`.\n",
        "    `h_t = f(W_h * h_{t-1} + W_x * x_t + b_h)`\n",
        "    where `W_h` and `W_x` are weight matrices, `b_h` is a bias vector, and `f` is the activation function.\n",
        "3.  The new hidden state `h_t` is then used to compute the output `y_t` at the current time step.\n",
        "    `y_t = g(W_y * h_t + b_y)`\n",
        "    where `W_y` is a weight matrix, `b_y` is a bias vector, and `g` is the activation function for the output layer.\n",
        "4.  The hidden state `h_t` is passed to the next time step `t+1`.\n",
        "\n",
        "This process is repeated for each element in the sequence. The recurrent connections allow the network to learn dependencies between elements in the sequence, even if they are far apart.\n",
        "\n",
        "In essence, RNNs are powerful for sequential data because of their ability to maintain a memory, making them suitable for tasks like language modeling, machine translation, speech recognition, and time series analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. **What is the key difference between LSTM and GRU networks in NLP applications?**"
      ],
      "metadata": {
        "id": "K5GGa7OxmnvV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b257e5d"
      },
      "source": [
        "Answer:\n",
        "\n",
        "**LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit) Networks**\n",
        "\n",
        "Both LSTM and GRU networks are types of recurrent neural networks (RNNs) designed to overcome the vanishing gradient problem that plagues simple RNNs when dealing with long sequences. They achieve this by using \"gating mechanisms\" that control the flow of information into and out of the memory cell. The key difference lies in the number and type of gates they use.\n",
        "\n",
        "**LSTM Networks:**\n",
        "\n",
        "LSTMs are more complex and use three main gates to regulate information:\n",
        "\n",
        "1.  **Forget Gate:** Decides what information to discard from the cell state.\n",
        "2.  **Input Gate:** Decides what new information to store in the cell state.\n",
        "3.  **Output Gate:** Decides what to output based on the cell state.\n",
        "\n",
        "These gates allow LSTMs to maintain a more complex memory and capture long-term dependencies effectively.\n",
        "\n",
        "**GRU Networks:**\n",
        "\n",
        "GRUs are a simplified version of LSTMs and use two main gates:\n",
        "\n",
        "1.  **Update Gate:** Acts as both the forget and input gate, deciding how much of the previous memory to keep and how much of the new information to add.\n",
        "2.  **Reset Gate:** Decides how much of the previous hidden state to forget.\n",
        "\n",
        "GRUs have fewer parameters than LSTMs, which can make them faster to train and require less data.\n",
        "\n",
        "**Key Difference Summary:**\n",
        "\n",
        "| Feature         | LSTM                                  | GRU                                     |\n",
        "| :-------------- | :------------------------------------ | :-------------------------------------- |\n",
        "| **Number of Gates**| 3 (Forget, Input, Output)             | 2 (Update, Reset)                       |\n",
        "| **Complexity**  | More complex, separate cell state      | Simpler, combines hidden state and cell state |\n",
        "| **Parameters**  | More parameters                       | Fewer parameters                        |\n",
        "| **Training Speed**| Generally slower                      | Generally faster                        |\n",
        "| **Performance** | Often performs well on complex tasks | Can perform comparably to LSTM with less data |\n",
        "\n",
        "In NLP applications, both LSTMs and GRUs are widely used for tasks like machine translation, text generation, and sentiment analysis. The choice between them often depends on the specific task, dataset size, and computational resources available. LSTMs might be preferred for tasks requiring a more nuanced memory, while GRUs can be a good choice when efficiency and fewer parameters are important."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. **Write a Python program using TextBlob to perform sentiment analysis on the following paragraph of text:**\n",
        "\n",
        "    “I had a great experience using the new mobile banking app. The interface is intuitive, and customer support was quick to resolve my issue. However, the app did crash once during a transaction, which was frustrating\"\n",
        "    \n",
        "    **Your program should print out the polarity and subjectivity scores.**"
      ],
      "metadata": {
        "id": "Ww1OuYDsm1bv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5f6f08a2",
        "outputId": "5c950aab-a34c-42fa-f93e-c5c57f93a523"
      },
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "text = \"I had a great experience using the new mobile banking app. The interface is intuitive, and customer support was quick to resolve my issue. However, the app did crash once during a transaction, which was frustrating\"\n",
        "\n",
        "# Create a TextBlob object\n",
        "blob = TextBlob(text)\n",
        "\n",
        "# Get the sentiment\n",
        "sentiment = blob.sentiment\n",
        "\n",
        "# Print the polarity and subjectivity scores\n",
        "print(f\"Polarity: {sentiment.polarity}\")\n",
        "print(f\"Subjectivity: {sentiment.subjectivity}\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Polarity: 0.21742424242424244\n",
            "Subjectivity: 0.6511363636363636\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. **Given the sample paragraph below, perform string tokenization and frequency distribution using Python and NLTK:**\n",
        "    \n",
        "    **“Natural Language Processing (NLP) is a fascinating field that combines linguistics, computer science, and artificial intelligence. It enables machines to understand, interpret, and generate human language. Applications of NLP include chatbots, sentiment analysis, and machine translation. As technology advances, the role of NLP in modern solutions is becoming increasingly critical.”**"
      ],
      "metadata": {
        "id": "v58ltw_gnVry"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "faf813db",
        "outputId": "db514470-6a01-4f78-f860-7be7cc2b494c"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ad5df93",
        "outputId": "7818f2cb-fe6a-4f09-f691-05afb33041f5"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "# Download necessary NLTK data (if not already downloaded)\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    nltk.download('punkt')\n",
        "    try:\n",
        "        nltk.data.find('tokenizers/punkt_tab')\n",
        "    except LookupError:\n",
        "        nltk.download('punkt_tab')\n",
        "\n",
        "\n",
        "text = \"Natural Language Processing (NLP) is a fascinating field that combines linguistics, computer science, and artificial intelligence. It enables machines to understand, interpret, and generate human language. Applications of NLP include chatbots, sentiment analysis, and machine translation. As technology advances, the role of NLP in modern solutions is becoming increasingly critical.\"\n",
        "\n",
        "# Perform tokenization\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Calculate frequency distribution\n",
        "fdist = FreqDist(tokens)\n",
        "\n",
        "# Print the tokens and frequency distribution\n",
        "print(\"Tokens:\")\n",
        "print(tokens)\n",
        "print(\"\\nFrequency Distribution:\")\n",
        "print(fdist.most_common(10)) # Print the 10 most common words"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:\n",
            "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', 'that', 'combines', 'linguistics', ',', 'computer', 'science', ',', 'and', 'artificial', 'intelligence', '.', 'It', 'enables', 'machines', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', '.', 'Applications', 'of', 'NLP', 'include', 'chatbots', ',', 'sentiment', 'analysis', ',', 'and', 'machine', 'translation', '.', 'As', 'technology', 'advances', ',', 'the', 'role', 'of', 'NLP', 'in', 'modern', 'solutions', 'is', 'becoming', 'increasingly', 'critical', '.']\n",
            "\n",
            "Frequency Distribution:\n",
            "[(',', 7), ('.', 4), ('NLP', 3), ('and', 3), ('is', 2), ('of', 2), ('Natural', 1), ('Language', 1), ('Processing', 1), ('(', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. **Implement a basic LSTM model in Keras for a text classification task using the following dummy dataset. Your model should classify sentences as either positive (1) or negative (0).**\n",
        "\n",
        "    **#Dataset**\n",
        "\n",
        "        texts = [\n",
        "                “I love this project”, #Positive\n",
        "                “This is an amazing experience”, #Positive\n",
        "                “I hate waiting in line”, #Negative\n",
        "                “This is the worst service”, #Negative\n",
        "                “Absolutely fantastic!” #Positive\n",
        "        ]\n",
        "    \n",
        "    **labels = [1, 1, 0, 0, 1]**\n",
        "    \n",
        "    **Preprocess the text, tokenize it, pad sequences, and build an LSTM model to train on this data. You may use Keras with TensorFlow backend.**"
      ],
      "metadata": {
        "id": "PArq80ybrSng"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "3af0af7c",
        "outputId": "b3bf25eb-589b-479f-9e84-a68b3a8f2e38"
      },
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# 1. Define the dataset\n",
        "texts = [\n",
        "    \"I love this project\",\n",
        "    \"This is an amazing experience\",\n",
        "    \"I hate waiting in line\",\n",
        "    \"This is the worst service\",\n",
        "    \"Absolutely fantastic!\"\n",
        "]\n",
        "\n",
        "labels = [1, 1, 0, 0, 1]\n",
        "\n",
        "# Convert labels to numpy array\n",
        "labels = np.array(labels)\n",
        "\n",
        "# 2. Preprocess the text\n",
        "# Tokenize the words\n",
        "tokenizer = Tokenizer(num_words=1000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "# 3. Pad sequences\n",
        "max_length = max([len(x) for x in sequences])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "# Define vocabulary size\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# 4. Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 16, input_length=max_length))\n",
        "model.add(LSTM(32))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# 5. Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n",
        "\n",
        "# 6. Train the model\n",
        "# For a real-world scenario, we would split data into training and testing sets.\n",
        "# Given this small dataset, we'll train on the whole dataset.\n",
        "history = model.fit(padded_sequences, labels, epochs=50, verbose=0) # verbose=0 to keep output clean\n",
        "\n",
        "# 7. Evaluate the model (Optional)\n",
        "loss, accuracy = model.evaluate(padded_sequences, labels, verbose=0)\n",
        "print(f\"\\nTraining Accuracy: {accuracy}\")\n",
        "\n",
        "# Example prediction (Optional)\n",
        "sample_text = [\"I had a terrible time\"]\n",
        "sample_sequence = tokenizer.texts_to_sequences(sample_text)\n",
        "sample_padded = pad_sequences(sample_sequence, maxlen=max_length, padding='post', truncating='post')\n",
        "prediction = model.predict(sample_padded)\n",
        "print(f\"\\nPrediction for '{sample_text[0]}': {prediction[0][0]}\")\n",
        "\n",
        "sample_text_2 = [\"I really enjoyed it\"]\n",
        "sample_sequence_2 = tokenizer.texts_to_sequences(sample_text_2)\n",
        "sample_padded_2 = pad_sequences(sample_sequence_2, maxlen=max_length, padding='post', truncating='post')\n",
        "prediction_2 = model.predict(sample_padded_2)\n",
        "print(f\"Prediction for '{sample_text_2[0]}': {prediction_2[0][0]}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Accuracy: 0.800000011920929\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step\n",
            "\n",
            "Prediction for 'I had a terrible time': 0.6379601955413818\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "Prediction for 'I really enjoyed it': 0.7144409418106079\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. **Using spaCy, build a simple NLP pipeline that includes tokenization, lemmatization, and entity recognition. Use the following paragraph as your dataset:**\n",
        "    \n",
        "    **“Homi Jehangir Bhaba was an Indian nuclear physicist who played a key role in the development of India’s atomic energy program. He was the founding director of the Tata Institute of Fundamental Research (TIFR) and was instrumental in establishing the Atomic Energy Commission of India.”**\n",
        "    \n",
        "    **Write a Python program that processes this text using spaCy, then prints tokens, their lemmas, and any named entities found.**"
      ],
      "metadata": {
        "id": "y-Pv6ubGt5HR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2284cf8c",
        "outputId": "73c49adb-ea2d-4571-dee9-62d290f11076"
      },
      "source": [
        "# Install spaCy\n",
        "#!pip install spacy\n",
        "\n",
        "# Download a spaCy language model\n",
        "!python -m spacy download en_core_web_sm\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Load the spaCy model\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    print(\"Downloading spaCy model 'en_core_web_sm'...\")\n",
        "    !python -m spacy download en_core_web_sm\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "text = \"Homi Jehangir Bhaba was an Indian nuclear physicist who played a key role in the development of India’s atomic energy program. He was the founding director of the Tata Institute of Fundamental Research (TIFR) and was instrumental in establishing the Atomic Energy Commission of India.\"\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Print tokens and their lemmas\n",
        "print(\"Tokens and Lemmas:\")\n",
        "for token in doc:\n",
        "    print(f\"Token: {token.text}, Lemma: {token.lemma_}\")\n",
        "\n",
        "# Print named entities\n",
        "print(\"\\nNamed Entities:\")\n",
        "for ent in doc.ents:\n",
        "    print(f\"Entity: {ent.text}, Label: {ent.label_}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m97.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Tokens and Lemmas:\n",
            "Token: Homi, Lemma: Homi\n",
            "Token: Jehangir, Lemma: Jehangir\n",
            "Token: Bhaba, Lemma: Bhaba\n",
            "Token: was, Lemma: be\n",
            "Token: an, Lemma: an\n",
            "Token: Indian, Lemma: indian\n",
            "Token: nuclear, Lemma: nuclear\n",
            "Token: physicist, Lemma: physicist\n",
            "Token: who, Lemma: who\n",
            "Token: played, Lemma: play\n",
            "Token: a, Lemma: a\n",
            "Token: key, Lemma: key\n",
            "Token: role, Lemma: role\n",
            "Token: in, Lemma: in\n",
            "Token: the, Lemma: the\n",
            "Token: development, Lemma: development\n",
            "Token: of, Lemma: of\n",
            "Token: India, Lemma: India\n",
            "Token: ’s, Lemma: ’s\n",
            "Token: atomic, Lemma: atomic\n",
            "Token: energy, Lemma: energy\n",
            "Token: program, Lemma: program\n",
            "Token: ., Lemma: .\n",
            "Token: He, Lemma: he\n",
            "Token: was, Lemma: be\n",
            "Token: the, Lemma: the\n",
            "Token: founding, Lemma: found\n",
            "Token: director, Lemma: director\n",
            "Token: of, Lemma: of\n",
            "Token: the, Lemma: the\n",
            "Token: Tata, Lemma: Tata\n",
            "Token: Institute, Lemma: Institute\n",
            "Token: of, Lemma: of\n",
            "Token: Fundamental, Lemma: Fundamental\n",
            "Token: Research, Lemma: Research\n",
            "Token: (, Lemma: (\n",
            "Token: TIFR, Lemma: TIFR\n",
            "Token: ), Lemma: )\n",
            "Token: and, Lemma: and\n",
            "Token: was, Lemma: be\n",
            "Token: instrumental, Lemma: instrumental\n",
            "Token: in, Lemma: in\n",
            "Token: establishing, Lemma: establish\n",
            "Token: the, Lemma: the\n",
            "Token: Atomic, Lemma: Atomic\n",
            "Token: Energy, Lemma: Energy\n",
            "Token: Commission, Lemma: Commission\n",
            "Token: of, Lemma: of\n",
            "Token: India, Lemma: India\n",
            "Token: ., Lemma: .\n",
            "\n",
            "Named Entities:\n",
            "Entity: Homi Jehangir Bhaba, Label: FAC\n",
            "Entity: Indian, Label: NORP\n",
            "Entity: India, Label: GPE\n",
            "Entity: the Tata Institute of Fundamental Research, Label: ORG\n",
            "Entity: the Atomic Energy Commission of India, Label: ORG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. **You are working on a chatbot for a mental health platform. Explain how you would leverage LSTM or GRU networks along with libraries like spaCy or Stanford NLP to understand and respond to user input effectively. Detail your architecture, data preprocessing pipeline, and any ethical considerations.**"
      ],
      "metadata": {
        "id": "OGVLDdfgucIn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer:"
      ],
      "metadata": {
        "id": "u-vx9Olrv7kT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6730f66e"
      },
      "source": [
        "### Chatbot Architecture with RNNs (LSTM/GRU)\n",
        "\n",
        "A mental health chatbot designed to understand and respond to user input effectively can leverage Recurrent Neural Networks (RNNs), specifically LSTMs or GRUs, due to their ability to process sequential data like text. Here's a possible architecture:\n",
        "\n",
        "**1. Input Layer:**\n",
        "   - This layer receives the user's text input.\n",
        "\n",
        "**2. Embedding Layer:**\n",
        "   - The input text is converted into numerical representations (vectors). Each word or token is mapped to a dense vector space where words with similar meanings are closer to each other. This is crucial for the model to understand the semantic relationships between words.\n",
        "\n",
        "**3. Recurrent Layers (LSTM or GRU):**\n",
        "   - This is the core of the network. One or more layers of LSTMs or GRUs process the sequence of embedded word vectors.\n",
        "   - **LSTM (Long Short-Term Memory):** LSTMs are well-suited for capturing long-term dependencies in text. They use internal gates (forget, input, and output gates) to control the flow of information, allowing them to remember relevant information over long sequences and forget irrelevant information.\n",
        "   - **GRU (Gated Recurrent Unit):** GRUs are a simpler variant of LSTMs with two gates (update and reset gates). They often perform comparably to LSTMs while having fewer parameters, making them faster to train on some datasets.\n",
        "   - These layers process the embedded sequence step by step, maintaining a hidden state that summarizes the information processed so far. This hidden state is passed to the next time step.\n",
        "\n",
        "**4. Dense Layers:**\n",
        "   - The output from the final recurrent layer is passed through one or more fully connected (dense) layers. These layers learn to map the hidden state representation to the desired output.\n",
        "\n",
        "**5. Output Layer:**\n",
        "   - The final dense layer produces the output. The nature of the output layer depends on the specific task the chatbot is performing:\n",
        "     - **Text Classification (e.g., intent recognition, sentiment analysis):** A softmax activation function is used to output a probability distribution over a set of predefined classes (e.g., \"seeking help,\" \"expressing sadness,\" \"neutral\").\n",
        "     - **Sequence Generation (e.g., generating a response):** A softmax activation function is used over the vocabulary to predict the next word in the response sequence. This often involves a more complex sequence-to-sequence architecture with an encoder-decoder structure.\n",
        "\n",
        "**Flow of Information:**\n",
        "\n",
        "User Input (Text) -> Input Layer -> Embedding Layer -> Recurrent Layers (LSTM/GRU) -> Dense Layers -> Output Layer -> Chatbot Response\n",
        "\n",
        "This architecture allows the chatbot to process the sequence of words in the user's input, understand the context, and generate a relevant response based on the learned patterns in the training data. The choice between LSTM and GRU depends on factors like dataset size, computational resources, and desired performance.\n",
        "\n",
        "### Data Preprocessing Pipeline\n",
        "\n",
        "Before feeding the text data into the RNN model, it needs to be preprocessed. This involves several steps to convert the raw text into a numerical format that the model can understand. Libraries like spaCy or Stanford NLP are invaluable for these steps.\n",
        "\n",
        "Here's a typical preprocessing pipeline:\n",
        "\n",
        "1.  **Text Cleaning:**\n",
        "    -   Remove unwanted characters (e.g., punctuation, special symbols, HTML tags).\n",
        "    -   Convert text to lowercase to ensure consistency.\n",
        "    -   Handle contractions and abbreviations (optional, depending on the dataset).\n",
        "\n",
        "2.  **Tokenization:**\n",
        "    -   Break down the text into individual words or sub-word units (tokens). This is a fundamental step in NLP. spaCy's `nlp()` object or NLTK's `word_tokenize` can be used for this.\n",
        "\n",
        "3.  **Lemmatization/Stemming:**\n",
        "    -   Reduce words to their base or root form. Lemmatization (using spaCy's `.lemma_` attribute or NLTK's WordNetLemmatizer) is generally preferred over stemming as it produces actual words, preserving meaning. This helps in reducing the vocabulary size and grouping similar words.\n",
        "\n",
        "4.  **Stop Word Removal:**\n",
        "    -   Remove common words that do not carry much meaning (e.g., \"the,\" \"a,\" \"is\"). spaCy and NLTK provide lists of stop words. This step can be optional depending on the task; for sentiment analysis, stop words might be important.\n",
        "\n",
        "5.  **Handling Out-of-Vocabulary (OOV) Words:**\n",
        "    -   Decide how to handle words that are not present in the model's vocabulary. This is especially important for unseen words during inference. Techniques include using a special `<OOV>` token or character-level embeddings.\n",
        "\n",
        "6.  **Vectorization/Encoding:**\n",
        "    -   Convert the cleaned and tokenized text into numerical vectors. Common methods include:\n",
        "        -   **One-Hot Encoding:** Each word is represented by a binary vector. Can lead to very high-dimensional sparse vectors.\n",
        "        -   **Word Embeddings:** Words are represented by dense vectors learned during training (e.g., using an Embedding layer in Keras). Pre-trained embeddings (like Word2Vec, GloVe, FastText) can also be used.\n",
        "        -   **TF-IDF:** Represents the importance of a word in a document relative to a corpus.\n",
        "\n",
        "7.  **Padding Sequences:**\n",
        "    -   Since RNNs typically require fixed-length inputs, sequences of different lengths need to be padded or truncated to a uniform length (`max_length`). Keras's `pad_sequences` function is useful for this.\n",
        "\n",
        "Libraries like spaCy integrate many of these steps into a single pipeline, making it efficient for production use. Stanford NLP's CoreNLP suite also provides a comprehensive set of tools for linguistic analysis.\n",
        "\n",
        "### Ethical Considerations for a Mental Health Chatbot\n",
        "\n",
        "Building a mental health chatbot comes with significant ethical responsibilities. It's crucial to prioritize user well-being, privacy, and safety. Here are some key considerations:\n",
        "\n",
        "1.  **Data Privacy and Security:**\n",
        "    -   User conversations contain highly sensitive personal information. Strong encryption, secure storage, and strict access controls are essential to protect data privacy.\n",
        "    -   Clearly communicate the data handling policies to users.\n",
        "    -   Comply with relevant data protection regulations (e.g., GDPR, HIPAA).\n",
        "\n",
        "2.  **Accuracy and Limitations:**\n",
        "    -   Chatbots are not a substitute for professional medical advice or therapy. Clearly state the limitations of the chatbot and emphasize that it is a supportive tool, not a diagnostic or treatment provider.\n",
        "    -   Ensure the information provided by the chatbot is accurate, evidence-based, and up-to-date.\n",
        "    -   Avoid making definitive diagnoses or providing medical recommendations.\n",
        "\n",
        "3.  **Bias in Data and Responses:**\n",
        "    -   Training data can contain biases that can be reflected in the chatbot's responses, potentially leading to discriminatory or harmful interactions.\n",
        "    -   Actively work to identify and mitigate bias in the training data and the model's outputs.\n",
        "    -   Regularly evaluate the chatbot's responses for fairness and equity.\n",
        "\n",
        "4.  **Handling Crisis Situations:**\n",
        "    -   The chatbot must be equipped to recognize and appropriately respond to users expressing suicidal ideation, self-harm, or other crises.\n",
        "    -   Provide immediate access to emergency resources (e.g., crisis hotlines, emergency services).\n",
        "    -   Have a clear protocol for escalating high-risk situations to human professionals if necessary.\n",
        "\n",
        "5.  **Transparency and Explainability:**\n",
        "    -   Be transparent with users about the fact that they are interacting with an AI.\n",
        "    -   While complex models like LSTMs can be black boxes, strive for as much explainability as possible in how the chatbot arrives at its responses.\n",
        "\n",
        "6.  **Human Oversight and Collaboration:**\n",
        "    -   A mental health chatbot should ideally work in conjunction with human mental health professionals.\n",
        "    -   Provide options for users to connect with a human therapist or counselor when needed.\n",
        "    -   Human oversight is crucial for monitoring chatbot interactions, identifying potential issues, and providing support in complex cases.\n",
        "\n",
        "7.  **Continuous Monitoring and Evaluation:**\n",
        "    -   Regularly monitor chatbot interactions to identify potential problems, areas for improvement, and instances where the chatbot might be providing unhelpful or harmful responses.\n",
        "    -   Continuously evaluate the chatbot's performance and update the model and content based on user feedback and expert review.\n",
        "\n",
        "Building an ethical mental health chatbot requires a multidisciplinary approach involving NLP experts, mental health professionals, and ethicists. The focus should always be on augmenting human care, not replacing it, and ensuring the safety and well-being of the users.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Leveraging LSTM or GRU networks within a thoughtful architecture and robust preprocessing pipeline, while strictly adhering to ethical guidelines, can lead to the development of mental health chatbots that provide valuable support to users. Remember that these tools are best used to augment human care and should always prioritize user safety and well-being."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58dfe686"
      },
      "source": [
        "### Python Code Examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d06ec1af",
        "outputId": "0bbf3cb6-9137-4695-ac4b-4c45a3ff3590"
      },
      "source": [
        "# Example: Text Preprocessing using spaCy\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Load the spaCy model\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except OSError:\n",
        "    print(\"Downloading spaCy model 'en_core_web_sm'...\")\n",
        "    !python -m spacy download en_core_web_sm\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"This is an example sentence demonstrating text preprocessing with spaCy. It includes punctuation, and words like running and better.\"\n",
        "\n",
        "# Process the text\n",
        "doc = nlp(text)\n",
        "\n",
        "# Tokenization, Lemmatization, and Stop Word Removal\n",
        "processed_tokens = []\n",
        "for token in doc:\n",
        "    if not token.is_punct and not token.is_stop:\n",
        "        processed_tokens.append(token.lemma_.lower()) # Convert to lowercase lemma\n",
        "\n",
        "print(\"Original Text:\", text)\n",
        "print(\"Processed Tokens (Lemma, no punctuation/stopwords):\", processed_tokens)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: This is an example sentence demonstrating text preprocessing with spaCy. It includes punctuation, and words like running and better.\n",
            "Processed Tokens (Lemma, no punctuation/stopwords): ['example', 'sentence', 'demonstrate', 'text', 'preprocesse', 'spacy', 'include', 'punctuation', 'word', 'like', 'run', 'well']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "b82d72c0",
        "outputId": "bd80998e-0a2e-4aa1-b992-bafe2d90b22f"
      },
      "source": [
        "# Example: Building a basic LSTM Model with Keras (for a hypothetical text classification task)\n",
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# Sample Data (replace with the actual mental health dataset)\n",
        "# This is a simplified example for demonstration\n",
        "texts = [\n",
        "    \"I feel very sad today\",\n",
        "    \"I am struggling with anxiety\",\n",
        "    \"I had a good day\",\n",
        "    \"Feeling hopeful about the future\",\n",
        "    \"I don't know what to do anymore\",\n",
        "    \"Talking to someone might help\"\n",
        "]\n",
        "\n",
        "# Hypothetical labels (e.g., 0 for negative sentiment, 1 for positive/neutral)\n",
        "labels = [0, 0, 1, 1, 0, 1]\n",
        "\n",
        "labels = np.array(labels)\n",
        "\n",
        "# Tokenization and Padding\n",
        "tokenizer = Tokenizer(num_words=1000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "max_length = max([len(x) for x in sequences])\n",
        "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 16, input_length=max_length)) # Embedding layer\n",
        "model.add(LSTM(32)) # LSTM layer\n",
        "model.add(Dense(1, activation='sigmoid')) # Output layer for binary classification\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Print model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model (using the small sample data - for demonstration)\n",
        "# In a real scenario, split data into training and validation sets\n",
        "history = model.fit(padded_sequences, labels, epochs=100, verbose=0)\n",
        "\n",
        "# Example of making a prediction\n",
        "sample_text = [\"I feel terrible\"]\n",
        "sample_sequence = tokenizer.texts_to_sequences(sample_text)\n",
        "sample_padded = pad_sequences(sample_sequence, maxlen=max_length, padding='post', truncating='post')\n",
        "prediction = model.predict(sample_padded)\n",
        "print(f\"\\nPrediction for '{sample_text[0]}': {prediction[0][0]}\") # Output is probability of class 1 (positive/neutral)\n",
        "\n",
        "sample_text_2 = [\"I am feeling better\"]\n",
        "sample_sequence_2 = tokenizer.texts_to_sequences(sample_text_2)\n",
        "sample_padded_2 = pad_sequences(sample_sequence_2, maxlen=max_length, padding='post', truncating='post')\n",
        "prediction_2 = model.predict(sample_padded_2)\n",
        "print(f\"Prediction for '{sample_text_2[0]}': {prediction_2[0][0]}\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step\n",
            "\n",
            "Prediction for 'I feel terrible': 0.032006267458200455\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "Prediction for 'I am feeling better': 0.10003579407930374\n"
          ]
        }
      ]
    }
  ]
}