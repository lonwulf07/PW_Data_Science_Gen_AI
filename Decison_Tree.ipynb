{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Decision Tree**"
      ],
      "metadata": {
        "id": "oL-mD2G9O0ko"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Theoretical Questions**"
      ],
      "metadata": {
        "id": "eDt72IJzO46D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **What is a Decision Tree, and how does it work?**\n",
        "\n",
        "Ans. A Decision Tree is a non-parametric supervised learning algorithm that is used for both classification and regression tasks. It essentially builds a model in the form of a tree structure, where each internal node represents a \"test\" on an attribute (feature), each branch represents the outcome of that test, and each leaf node represents a class label (for classification) or a numerical value (for regression).\n",
        "\n",
        "How it Works:\n",
        "  \n",
        "The fundamental idea behind a Decision Tree algorithm is to recursively split the dataset into smaller, more homogeneous subsets based on the values of the input features. This process continues until the subsets are \"pure\" enough (meaning most or all data points in that subset belong to the same class or have similar values for regression) or a stopping criterion is met.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "JBJHhSPHO8ID"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **What are impurity measures in Decision Trees?**\n",
        "\n",
        "Ans. In Decision Trees, impurity measures (also known as attribute selection measures) are crucial for determining how to best split a node. The goal of a decision tree is to create nodes that are as \"pure\" as possible, meaning that the majority (or all) of the data points within that node belong to the same class (for classification) or have very similar values (for regression).\n",
        "\n",
        "These impurity measures quantify the homogeneity or heterogeneity of a set of data points. When building a decision tree, the algorithm evaluates different potential splits based on various features and selects the split that results in the greatest reduction in impurity (or the highest \"information gain\").\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "UC8-wrXSPI_I"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a2f55a7"
      },
      "source": [
        "3. **What is the mathematical formula for Gini Impurity?**\n",
        "\n",
        "Ans. The Gini Impurity for a dataset $D$ is calculated as:\n",
        "\n",
        "$$Gini(D) = 1 - \\sum_{i=1}^{c} (p_i)^2$$\n",
        "\n",
        "Where:\n",
        "- $c$ is the number of classes.\n",
        "- $p_i$ is the proportion of instances in dataset $D$ that belong to class $i$.\n",
        "\n",
        "The Gini Impurity of a split is calculated as the weighted average of the Gini Impurity of the resulting subsets. For a split on attribute $A$ that divides dataset $D$ into subsets $D_1, D_2, ..., D_k$, the Gini Impurity of the split is:\n",
        "\n",
        "$$Gini_{split}(D) = \\sum_{j=1}^{k} \\frac{|D_j|}{|D|} Gini(D_j)$$\n",
        "\n",
        "The goal is to choose the split that minimizes the Gini Impurity of the split.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2554e01"
      },
      "source": [
        "4. **What is the mathematical formula for Entropy?**\n",
        "\n",
        "Ans. Entropy is another impurity measure used in Decision Trees, particularly in algorithms like ID3 and C4.5. It quantifies the amount of uncertainty or randomness in a dataset. A pure dataset (where all instances belong to the same class) has zero entropy, while a dataset with an equal distribution of classes has maximum entropy.\n",
        "\n",
        "The mathematical formula for Entropy for a dataset $D$ is:\n",
        "\n",
        "$$Entropy(D) = - \\sum_{i=1}^{c} p_i \\log_2(p_i)$$\n",
        "\n",
        "Where:\n",
        "- $c$ is the number of classes.\n",
        "- $p_i$ is the proportion of instances in dataset $D$ that belong to class $i$.\n",
        "\n",
        "The Information Gain of a split is calculated as the reduction in entropy after the split. For a split on attribute $A$ that divides dataset $D$ into subsets $D_1, D_2, ..., D_k$, the Information Gain is:\n",
        "\n",
        "$$Information Gain(D, A) = Entropy(D) - \\sum_{j=1}^{k} \\frac{|D_j|}{|D|} Entropy(D_j)$$\n",
        "\n",
        "The goal is to choose the split that maximizes the Information Gain.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ee38df5"
      },
      "source": [
        "5. **What is Information Gain, and how is it used in Decision Trees?**\n",
        "\n",
        "Ans. Information Gain is a measure used in Decision Trees, particularly in algorithms like ID3 and C4.5, to determine the effectiveness of splitting a node based on a particular attribute. It quantifies the reduction in entropy (or uncertainty) achieved by splitting the dataset based on that attribute.\n",
        "\n",
        "How it is used:\n",
        "\n",
        "When building a Decision Tree, the algorithm evaluates different potential splits based on various features. For each potential split, it calculates the Information Gain. The split that results in the highest Information Gain is chosen as the best split for that node. This process is repeated recursively for each resulting child node until a stopping criterion is met. In essence, the algorithm greedily selects the splits that provide the most \"information\" about the target variable, leading to more homogeneous subsets and a more accurate tree.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bb4e20e"
      },
      "source": [
        "6. **What is the difference between Gini Impurity and Entropy?**\n",
        "\n",
        "Ans. Both Gini Impurity and Entropy are measures used in Decision Trees to evaluate the homogeneity of a dataset and determine the best split. While they serve the same purpose, they differ in their mathematical formulas and interpretations:\n",
        "\n",
        "**Gini Impurity:**\n",
        "\n",
        "- **Formula:** $Gini(D) = 1 - \\sum_{i=1}^{c} (p_i)^2$\n",
        "- **Interpretation:** Gini Impurity measures the probability of incorrectly classifying a randomly chosen element in the dataset if it were randomly labeled according to the distribution of labels in the subset. A lower Gini Impurity indicates a more homogeneous dataset.\n",
        "- **Calculation:** It involves squaring the probabilities of each class.\n",
        "- **Bias:** It tends to isolate the most frequent class in its own branch of the tree.\n",
        "- **Computational Cost:** Generally computationally less expensive than Entropy as it doesn't involve logarithms.\n",
        "\n",
        "**Entropy:**\n",
        "\n",
        "- **Formula:** $Entropy(D) = - \\sum_{i=1}^{c} p_i \\log_2(p_i)$\n",
        "- **Interpretation:** Entropy measures the amount of uncertainty or randomness in a dataset. A lower Entropy indicates a more homogeneous dataset with less uncertainty.\n",
        "- **Calculation:** It involves the logarithm of the probabilities of each class.\n",
        "- **Bias:** It tends to produce a more balanced tree.\n",
        "- **Computational Cost:** Generally computationally more expensive than Gini Impurity due to the logarithm calculation.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75d531ed"
      },
      "source": [
        "7. **What is the mathematical explanation behind Decision Trees?**\n",
        "\n",
        "Ans. The mathematical basis of Decision Trees lies in recursively partitioning the feature space based on the values of the input features to create regions where the instances are as homogeneous as possible with respect to the target variable. This partitioning process is guided by impurity measures (like Gini Impurity or Entropy) and the concept of Information Gain.\n",
        "\n",
        "Here's a breakdown of the key mathematical aspects:\n",
        "\n",
        "**1. Impurity Measures (Gini Impurity and Entropy):**\n",
        "\n",
        "As discussed in previous questions, these measures quantify the homogeneity of a dataset. The lower the impurity, the more homogeneous the data is. Mathematically, they are calculated as:\n",
        "\n",
        "- **Gini Impurity:** $Gini(D) = 1 - \\sum_{i=1}^{c} (p_i)^2$\n",
        "- **Entropy:** $Entropy(D) = - \\sum_{i=1}^{c} p_i \\log_2(p_i)$\n",
        "\n",
        "The goal during tree construction is to minimize the impurity at each node after splitting.\n",
        "\n",
        "**2. Information Gain:**\n",
        "\n",
        "Information Gain measures the reduction in impurity achieved by splitting a node based on a particular attribute. It's calculated as the difference between the impurity of the parent node and the weighted average of the impurity of the child nodes:\n",
        "\n",
        "$$Information Gain(D, A) = Impurity(D) - \\sum_{j=1}^{k} \\frac{|D_j|}{|D|} Impurity(D_j)$$\n",
        "\n",
        "Where $Impurity$ can be either Gini Impurity or Entropy. The algorithm selects the attribute and split point that maximizes Information Gain. This greedy approach aims to find the most informative splits at each step.\n",
        "\n",
        "**3. Recursive Partitioning:**\n",
        "\n",
        "The process starts with the root node, which represents the entire dataset. The algorithm calculates the Information Gain for all possible splits on all features and selects the split that yields the highest Information Gain. This splits the dataset into subsets, and child nodes are created for each subset. The process is then recursively applied to each child node until a stopping criterion is met (e.g., the nodes are pure, a maximum depth is reached, or the number of instances in a node is below a threshold).\n",
        "\n",
        "**4. Split Points for Continuous Features:**\n",
        "\n",
        "For continuous features, the algorithm needs to find the optimal split point. This is typically done by considering all possible split points between consecutive unique values of the feature. For each potential split point, the data is divided into two subsets, and the Information Gain is calculated. The split point that maximizes Information Gain is chosen.\n",
        "\n",
        "**5. Prediction:**\n",
        "\n",
        "Once the tree is built, prediction is made by traversing the tree from the root node down to a leaf node. At each internal node, the decision is made based on the value of the corresponding feature. When a leaf node is reached, the prediction is made based on the class label (for classification) or the average value (for regression) of the instances in that leaf node.\n",
        "\n",
        "In essence, the mathematical foundation of Decision Trees involves using impurity measures and Information Gain to guide the recursive partitioning of the feature space, creating a tree structure that effectively separates instances belonging to different classes or having different target values.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5dee0c3"
      },
      "source": [
        "8. **What is Pre-Pruning in Decision Trees?**\n",
        "\n",
        "Ans. Pre-pruning (also known as early stopping) is a technique used in Decision Trees to prevent overfitting by stopping the tree building process prematurely. Instead of growing the tree to its full potential (where all leaf nodes are pure or contain a single instance), pre-pruning sets criteria to stop splitting a node before it becomes perfectly pure.\n",
        "\n",
        "These criteria can include:\n",
        "\n",
        "- **Maximum depth of the tree:** Stop splitting once a certain depth is reached.\n",
        "- **Minimum number of instances required to split a node:** Prevent splitting nodes that have very few instances.\n",
        "- **Minimum number of instances required at a leaf node:** Ensure that leaf nodes have a certain number of instances.\n",
        "- **Maximum number of leaf nodes:** Limit the total number of leaf nodes in the tree.\n",
        "- **Impurity threshold:** Stop splitting if the impurity of a node is below a certain threshold.\n",
        "- **Lack of significant information gain:** Stop splitting if the gain from splitting a node is below a certain threshold.\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "During the tree construction phase, at each node, the algorithm checks if any of the pre-pruning criteria are met. If a criterion is met, the splitting process for that node is stopped, and the node becomes a leaf node. The class label (for classification) or the average value (for regression) of the instances in that node is assigned to the leaf.\n",
        "\n",
        "**Advantages of Pre-Pruning:**\n",
        "\n",
        "- **Reduces overfitting:** By stopping the tree growth early, it prevents the tree from learning noise in the training data.\n",
        "- **Speeds up training:** Building a smaller tree is computationally less expensive.\n",
        "- **Simplifies the tree:** Results in a more interpretable tree.\n",
        "\n",
        "**Disadvantages of Pre-Pruning:**\n",
        "\n",
        "- **May underfit:** Stopping the tree growth too early might prevent the tree from capturing complex relationships in the data.\n",
        "- **Difficulty in choosing optimal parameters:** Determining the right pre-pruning parameters can be challenging and often requires experimentation or cross-validation.\n",
        "\n",
        "In essence, pre-pruning acts as a constraint on the tree building process, aiming to find a balance between capturing the underlying patterns in the data and avoiding the complexity that can lead to overfitting.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bfacf84"
      },
      "source": [
        "9. **What is Post-Pruning in Decision Trees?**\n",
        "\n",
        "Ans. Post-pruning (also known as backward pruning) is a technique used in Decision Trees to prevent overfitting. Unlike pre-pruning, which stops the tree building process early, post-pruning allows the tree to grow to its full potential (until all leaf nodes are pure or contain a single instance) and then prunes (removes) branches that are not contributing significantly to the model's performance on unseen data.\n",
        "\n",
        "**How it works:**\n",
        "\n",
        "After the Decision Tree has been fully grown, post-pruning techniques evaluate the impact of removing certain branches or nodes. The goal is to remove parts of the tree that are likely capturing noise in the training data rather than the underlying patterns. This evaluation is typically done using a separate validation dataset or through cross-validation.\n",
        "\n",
        "Common post-pruning techniques include:\n",
        "\n",
        "- **Reduced Error Pruning:** This technique uses a validation set to evaluate the performance of the tree before and after pruning a branch. A branch is pruned if removing it does not increase the error on the validation set. This is a simple and effective method but requires a separate validation set.\n",
        "- **Cost-Complexity Pruning (Weakest Link Pruning):** This technique uses a complexity parameter ($\\alpha$) to penalize the number of leaf nodes. It finds a sequence of optimally pruned trees for different values of $\\alpha$ and selects the tree that performs best on a validation set or through cross-validation.\n",
        "\n",
        "**Advantages of Post-Pruning:**\n",
        "\n",
        "- **Often leads to better performance than pre-pruning:** Because it allows the tree to explore all possible splits first, it can potentially find better structures.\n",
        "- **Less prone to the \"horizon effect\":** Pre-pruning might stop the tree early and miss out on potentially good splits that would have appeared later in the tree. Post-pruning avoids this by building the full tree first.\n",
        "\n",
        "**Disadvantages of Post-Pruning:**\n",
        "\n",
        "- **Computationally more expensive:** Building a full tree and then pruning it takes more time than stopping early.\n",
        "- **Requires a separate validation set (for some techniques):** This reduces the amount of data available for training.\n",
        "\n",
        "In essence, post-pruning is a \"learn then prune\" approach that aims to find a simpler tree structure that generalizes well to new data by removing branches that are likely overfitting to the training data.\n",
        "\n",
        "* * *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4b4e4cd"
      },
      "source": [
        "10. **What is the difference between Pre-Pruning and Post-Pruning?**\n",
        "\n",
        "Ans. Both Pre-pruning and Post-pruning are techniques used to prevent overfitting in Decision Trees, but they differ in when and how they are applied:\n",
        "\n",
        "**Pre-Pruning (Early Stopping):**\n",
        "\n",
        "- **When applied:** During the tree building process.\n",
        "- **How it works:** It sets criteria to stop the tree growth prematurely before it becomes perfectly pure.\n",
        "- **Criteria examples:** Maximum depth, minimum instances per split/leaf, impurity threshold, lack of significant information gain.\n",
        "- **Advantages:** Faster training, simpler tree, reduces overfitting.\n",
        "- **Disadvantages:** May underfit, difficult to choose optimal parameters, can suffer from the \"horizon effect\" (stopping early and missing good future splits).\n",
        "\n",
        "**Post-Pruning (Backward Pruning):**\n",
        "\n",
        "- **When applied:** After the tree has been fully grown.\n",
        "- **How it works:** It prunes (removes) branches or nodes from the fully grown tree that are not contributing significantly to performance on unseen data.\n",
        "- **Techniques examples:** Reduced Error Pruning, Cost-Complexity Pruning.\n",
        "- **Advantages:** Often leads to better performance, less prone to the \"horizon effect.\"\n",
        "- **Disadvantages:** Computationally more expensive, may require a separate validation set.\n",
        "\n",
        "In essence, pre-pruning is a \"stop early\" approach, while post-pruning is a \"grow then cut\" approach. The choice between the two depends on factors like the size of the dataset, computational resources, and the desired trade-off between model complexity and performance.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fbe84c30"
      },
      "source": [
        "11. **What is a Decision Tree Regressor?**\n",
        "\n",
        "Ans. A Decision Tree Regressor is a supervised learning algorithm that uses a tree-like structure to predict a continuous target variable. It works similarly to a Decision Tree Classifier, but instead of predicting a class label at the leaf nodes, it predicts a numerical value.\n",
        "\n",
        "**How it Works:**\n",
        "\n",
        "The process of building a Decision Tree Regressor is similar to building a Decision Tree Classifier, but with differences in how splits are evaluated and how predictions are made at the leaf nodes:\n",
        "\n",
        "1. **Splitting Criteria:** Instead of impurity measures like Gini Impurity or Entropy (which are designed for classification), Decision Tree Regressors use measures that quantify the homogeneity of continuous values. Common splitting criteria include:\n",
        "    - **Mean Squared Error (MSE):** Measures the average of the squared differences between the actual and predicted values in a node. The goal is to minimize MSE after a split.\n",
        "    - **Mean Absolute Error (MAE):** Measures the average of the absolute differences between the actual and predicted values in a node. The goal is to minimize MAE after a split.\n",
        "\n",
        "2. **Recursive Partitioning:** The algorithm recursively splits the dataset into smaller subsets based on the values of the input features, just like in classification. The split that minimizes the chosen splitting criterion (e.g., MSE or MAE) is selected at each node.\n",
        "\n",
        "3. **Leaf Node Prediction:** Once a stopping criterion is met (e.g., maximum depth, minimum number of instances in a node), the node becomes a leaf node. The prediction for a new instance that falls into this leaf node is typically the average (mean) or median of the target variable values of the instances in that leaf node.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. **What are the advantages and disadvantages of Decision Trees?**\n",
        "\n",
        "Ans. **Advantages of Decision Tree Regressors:**\n",
        "\n",
        "- **Easy to understand and interpret:** The tree structure is intuitive.\n",
        "- **Can handle both numerical and categorical features:** Although categorical features may need to be encoded.\n",
        "- **Can capture non-linear relationships:** Can model complex relationships between features and the target variable.\n",
        "- **No need for feature scaling:** Decision trees are not sensitive to the scale of the features.\n",
        "\n",
        "**Disadvantages of Decision Tree Regressors:**\n",
        "\n",
        "- **Prone to overfitting:** Can create overly complex trees that fit the training data too well but generalize poorly to new data. Pruning techniques (pre-pruning and post-pruning) are used to mitigate this.\n",
        "- **Instability:** Small changes in the training data can lead to significantly different tree structures.\n",
        "- **Bias towards features with more levels:** Features with a larger number of unique values can be favored in the splitting process.\n",
        "\n",
        "In summary, a Decision Tree Regressor is a versatile algorithm for regression tasks that builds a tree by recursively partitioning the data based on features to predict a continuous target value at the leaf nodes.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "vQ8GUrJ_U38y"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "495b0947"
      },
      "source": [
        "13. **How does a Decision Tree handle missing values?**\n",
        "\n",
        "Ans. The way Decision Trees handle missing values can vary depending on the specific algorithm and implementation. Some common strategies include:\n",
        "\n",
        "- **Ignoring instances with missing values:** The simplest approach is to exclude instances that have missing values for the attribute being considered for a split. This can be problematic if there are many instances with missing values, as it can significantly reduce the size of the dataset used for splitting.\n",
        "\n",
        "- **Assigning missing values to the most frequent category or mean/median:** Missing categorical values can be assigned to the most frequent category in the node, while missing numerical values can be replaced with the mean or median of the existing values in the node. This is a simple imputation method but can introduce bias.\n",
        "\n",
        "- **Probabilistic assignment:** Missing values can be assigned to different branches of the split based on the probability distribution of the existing values in the node. For example, if a node is split based on a categorical feature and 70% of the non-missing instances have value 'A' and 30% have value 'B', an instance with a missing value for this feature might be sent down the 'A' branch with a probability of 0.7 and down the 'B' branch with a probability of 0.3.\n",
        "\n",
        "- **Surrogate Splits:** Some algorithms (like C4.5 and CART) use surrogate splits. If an instance has a missing value for the primary splitting attribute, the algorithm looks for a \"surrogate\" attribute that is highly correlated with the primary attribute and uses that surrogate attribute to decide which branch the instance should go down. This is a more sophisticated approach that tries to retain instances with missing values.\n",
        "\n",
        "- **Treating missing values as a separate category:** For categorical features, missing values can be treated as a distinct category.\n",
        "\n",
        "- **Algorithms that inherently handle missing values:** Some Decision Tree algorithms are designed to handle missing values directly without requiring explicit imputation. For example, in some implementations, when splitting on an attribute, instances with missing values for that attribute might be sent down both branches or a separate branch.\n",
        "\n",
        "The choice of method for handling missing values can impact the performance and structure of the Decision Tree. It's important to be aware of how the specific Decision Tree implementation you are using handles missing values and to consider appropriate pre-processing steps if necessary.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43fcbb28"
      },
      "source": [
        "14. **How does a Decision Tree handle categorical features?**\n",
        "\n",
        "Ans. Decision Trees can handle categorical features directly, but the specific approach depends on the algorithm and implementation. Here are common ways Decision Trees deal with categorical features:\n",
        "\n",
        "- **Binary Splits for Nominal Features:** For nominal (unordered) categorical features with multiple categories, the Decision Tree typically considers splitting the data into two subsets based on whether an instance belongs to a specific category or a set of categories. For example, if a feature is \"Color\" with categories \"Red\", \"Blue\", \"Green\", the tree might consider splits like \"Color is Red\" vs. \"Color is not Red\", or \"Color is {Red, Blue}\" vs. \"Color is {Green}\". The algorithm evaluates all possible binary splits and chooses the one that maximizes the information gain or minimizes impurity.\n",
        "\n",
        "- **Multi-way Splits (less common):** Some algorithms (like the original ID3) can create multi-way splits for nominal features, where each branch corresponds to a unique category of the feature. However, this can lead to very bushy trees and is not as common in modern implementations like CART (used in scikit-learn), which primarily uses binary splits.\n",
        "\n",
        "- **Handling Ordinal Features:** For ordinal (ordered) categorical features (e.g., \"Small\", \"Medium\", \"Large\"), the Decision Tree can maintain the order and consider splits based on thresholds, similar to how it handles numerical features. For example, it might split based on \"Size is Medium or Large\" vs. \"Size is Small\".\n",
        "\n",
        "- **Encoding Categorical Features:** In some cases, especially with implementations that don't natively handle categorical features with multiple categories efficiently (like the CART algorithm which prefers binary splits), you might need to encode categorical features before feeding them to the Decision Tree. Common encoding techniques include:\n",
        "    - **One-Hot Encoding:** Creates binary dummy variables for each category. This can increase the dimensionality of the data but is a common practice.\n",
        "    - **Label Encoding:** Assigns a unique integer to each category. This should generally only be used for ordinal features where the integer order reflects the category order, as it can introduce an artificial sense of order to nominal features that the tree might misinterpret.\n",
        "\n",
        "- **Algorithm-Specific Handling:** Different Decision Tree algorithms have different ways of handling categorical features. For example, C4.5 can handle multi-way splits and missing values more gracefully than the basic ID3 algorithm.\n",
        "\n",
        "In summary, while Decision Trees can work with categorical features, the exact method varies. Modern implementations often rely on binary splits for nominal features and may benefit from pre-processing techniques like one-hot encoding for features with a large number of categories.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f142011d"
      },
      "source": [
        "15. **What are some real-world applications of Decision Trees?**\n",
        "\n",
        "Ans. Decision Trees are versatile and widely used in various real-world applications across different domains due to their interpretability and ability to handle both classification and regression tasks. Here are some examples:\n",
        "\n",
        "- **Medical Diagnosis:** Decision Trees can be used to build models that help diagnose diseases based on patient symptoms, medical history, and test results.\n",
        "- **Credit Risk Assessment:** Banks and financial institutions use Decision Trees to assess the creditworthiness of loan applicants based on their financial information and historical data.\n",
        "- **Customer Relationship Management (CRM):** Decision Trees can help businesses analyze customer data to identify potential churners, segment customers, and personalize marketing campaigns.\n",
        "- **Fraud Detection:** Decision Trees are used to detect fraudulent transactions in various industries, including banking, insurance, and e-commerce, by identifying suspicious patterns in transaction data.\n",
        "- **Spam Filtering:** Decision Trees can be used to classify emails as spam or not spam based on the content and characteristics of the email.\n",
        "- **Drug Discovery:** Decision Trees can help researchers identify potential drug candidates by analyzing the properties of different compounds and their effects.\n",
        "- **Manufacturing and Quality Control:** Decision Trees can be used to identify the factors that contribute to defects in manufacturing processes and to build models for quality control.\n",
        "- **Recommendation Systems:** Decision Trees can be used in recommendation systems to predict user preferences and recommend products or content based on their past behavior and characteristics.\n",
        "- **Natural Language Processing (NLP):** Decision Trees can be used in various NLP tasks, such as sentiment analysis, text classification, and part-of-speech tagging.\n",
        "- **Image Recognition:** While deep learning is dominant in image recognition, Decision Trees can still be used in some image processing tasks, particularly in combination with other techniques.\n",
        "- **Ecological Modeling:** Decision Trees are used to model the distribution of species and predict the impact of environmental factors on ecosystems.\n",
        "- **Financial Market Analysis:** Decision Trees can be used to analyze financial data and predict stock prices or market trends.\n",
        "\n",
        "These are just a few examples, and the applications of Decision Trees continue to expand as more data becomes available and computational power increases. Their transparency and ease of understanding make them a valuable tool in many decision-making processes.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Practical Questions**"
      ],
      "metadata": {
        "id": "_5fM_JruVTqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. **Write a Python program to train a Decision Tree Classifier on the Iris dataset and print the model accuracy.**"
      ],
      "metadata": {
        "id": "uwLTVvgBVW9n"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2f8d44f",
        "outputId": "a24076f1-9b3f-4b4d-d3f9-2c18c7a26900"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier()\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the Decision Tree Classifier on the Iris dataset: {accuracy:.2f}\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the Decision Tree Classifier on the Iris dataset: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. **Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances.**"
      ],
      "metadata": {
        "id": "NzEXzemPVr6O"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac2a251a",
        "outputId": "fbfec757-4632-4b6b-e25e-b2fb8821a377"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier using Gini Impurity\n",
        "clf_gini = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "clf_gini.fit(X_train, y_train)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances (Gini Impurity):\")\n",
        "for i, importance in enumerate(clf_gini.feature_importances_):\n",
        "    print(f\"{iris.feature_names[i]}: {importance:.4f}\")\n",
        "\n",
        "# Optional: Make predictions and print accuracy (similar to previous example)\n",
        "# y_pred_gini = clf_gini.predict(X_test)\n",
        "# accuracy_gini = accuracy_score(y_test, y_pred_gini)\n",
        "# print(f\"\\nAccuracy of the Decision Tree Classifier (Gini) on the Iris dataset: {accuracy_gini:.2f}\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances (Gini Impurity):\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0191\n",
            "petal length (cm): 0.8933\n",
            "petal width (cm): 0.0876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "18 **Write a Python program to train a Decision Tree Classifier using Entropy as the splitting criterion and print the model accuracy.**"
      ],
      "metadata": {
        "id": "8NaXWri7V28-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f82e437",
        "outputId": "47e0e538-3988-4d3e-e028-be6b6314d626"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier using Entropy\n",
        "clf_entropy = DecisionTreeClassifier(criterion='entropy', random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "clf_entropy.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_entropy = clf_entropy.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy_entropy = accuracy_score(y_test, y_pred_entropy)\n",
        "print(f\"Accuracy of the Decision Tree Classifier (Entropy) on the Iris dataset: {accuracy_entropy:.2f}\")\n",
        "\n",
        "# Optional: Print feature importances (similar to previous example)\n",
        "# print(\"\\nFeature Importances (Entropy):\")\n",
        "# for i, importance in enumerate(clf_entropy.feature_importances_):\n",
        "#     print(f\"{iris.feature_names[i]}: {importance:.4f}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the Decision Tree Classifier (Entropy) on the Iris dataset: 0.98\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. **Write a Python program to train a Decision Tree Regressor on a housing dataset and evaluate using Mean Squared Error (MSE).**"
      ],
      "metadata": {
        "id": "n4CXjBjHWZwm"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b30fadd8",
        "outputId": "fc86a2a8-21cb-41e5-f4a4-a6b6cfb364ba"
      },
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Train the regressor\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Calculate and print the Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error (MSE) of the Decision Tree Regressor on the housing dataset: {mse:.2f}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE) of the Decision Tree Regressor on the housing dataset: 0.53\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. **Write a Python program to train a Decision Tree Classifier and visualize the tree using graphviz.**"
      ],
      "metadata": {
        "id": "JDD3TxPpZK66"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c514f616",
        "outputId": "1cc6257c-d702-4a15-ba15-05f3e3a2fbbd"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "from sklearn.model_selection import train_test_split\n",
        "import graphviz\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets (optional, but good practice)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Export the decision tree to a DOT file\n",
        "dot_data = export_graphviz(clf, out_file=None,\n",
        "                           feature_names=iris.feature_names,\n",
        "                           class_names=iris.target_names,\n",
        "                           filled=True, rounded=True,\n",
        "                           special_characters=True)\n",
        "\n",
        "# Create a graph from the DOT data\n",
        "graph = graphviz.Source(dot_data)\n",
        "\n",
        "# Display the graph\n",
        "display(graph)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Generated by graphviz version 2.43.0 (0)\n -->\n<!-- Title: Tree Pages: 1 -->\n<svg width=\"749pt\" height=\"790pt\"\n viewBox=\"0.00 0.00 749.00 790.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 786)\">\n<title>Tree</title>\n<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-786 745,-786 745,4 -4,4\"/>\n<!-- 0 -->\n<g id=\"node1\" class=\"node\">\n<title>0</title>\n<path fill=\"#ffffff\" stroke=\"black\" d=\"M265,-782C265,-782 130,-782 130,-782 124,-782 118,-776 118,-770 118,-770 118,-711 118,-711 118,-705 124,-699 130,-699 130,-699 265,-699 265,-699 271,-699 277,-705 277,-711 277,-711 277,-770 277,-770 277,-776 271,-782 265,-782\"/>\n<text text-anchor=\"start\" x=\"126\" y=\"-766.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 2.45</text>\n<text text-anchor=\"start\" x=\"162\" y=\"-751.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.664</text>\n<text text-anchor=\"start\" x=\"152.5\" y=\"-736.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 105</text>\n<text text-anchor=\"start\" x=\"139.5\" y=\"-721.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [31, 37, 37]</text>\n<text text-anchor=\"start\" x=\"145\" y=\"-706.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 1 -->\n<g id=\"node2\" class=\"node\">\n<title>1</title>\n<path fill=\"#e58139\" stroke=\"black\" d=\"M166,-655.5C166,-655.5 73,-655.5 73,-655.5 67,-655.5 61,-649.5 61,-643.5 61,-643.5 61,-599.5 61,-599.5 61,-593.5 67,-587.5 73,-587.5 73,-587.5 166,-587.5 166,-587.5 172,-587.5 178,-593.5 178,-599.5 178,-599.5 178,-643.5 178,-643.5 178,-649.5 172,-655.5 166,-655.5\"/>\n<text text-anchor=\"start\" x=\"91.5\" y=\"-640.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"78.5\" y=\"-625.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 31</text>\n<text text-anchor=\"start\" x=\"69\" y=\"-610.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [31, 0, 0]</text>\n<text text-anchor=\"start\" x=\"76\" y=\"-595.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = setosa</text>\n</g>\n<!-- 0&#45;&gt;1 -->\n<g id=\"edge1\" class=\"edge\">\n<title>0&#45;&gt;1</title>\n<path fill=\"none\" stroke=\"black\" d=\"M170.44,-698.91C162.93,-687.65 154.78,-675.42 147.24,-664.11\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"150.07,-662.05 141.61,-655.67 144.25,-665.93 150.07,-662.05\"/>\n<text text-anchor=\"middle\" x=\"136.71\" y=\"-676.48\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n</g>\n<!-- 2 -->\n<g id=\"node3\" class=\"node\">\n<title>2</title>\n<path fill=\"#ffffff\" stroke=\"black\" d=\"M343,-663C343,-663 208,-663 208,-663 202,-663 196,-657 196,-651 196,-651 196,-592 196,-592 196,-586 202,-580 208,-580 208,-580 343,-580 343,-580 349,-580 355,-586 355,-592 355,-592 355,-651 355,-651 355,-657 349,-663 343,-663\"/>\n<text text-anchor=\"start\" x=\"204\" y=\"-647.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 4.75</text>\n<text text-anchor=\"start\" x=\"247.5\" y=\"-632.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\n<text text-anchor=\"start\" x=\"234.5\" y=\"-617.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 74</text>\n<text text-anchor=\"start\" x=\"221\" y=\"-602.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 37, 37]</text>\n<text text-anchor=\"start\" x=\"223\" y=\"-587.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 0&#45;&gt;2 -->\n<g id=\"edge2\" class=\"edge\">\n<title>0&#45;&gt;2</title>\n<path fill=\"none\" stroke=\"black\" d=\"M224.56,-698.91C230.43,-690.1 236.7,-680.7 242.76,-671.61\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"245.85,-673.28 248.49,-663.02 240.03,-669.4 245.85,-673.28\"/>\n<text text-anchor=\"middle\" x=\"253.39\" y=\"-683.84\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n</g>\n<!-- 3 -->\n<g id=\"node4\" class=\"node\">\n<title>3</title>\n<path fill=\"#3fe685\" stroke=\"black\" d=\"M252.5,-544C252.5,-544 130.5,-544 130.5,-544 124.5,-544 118.5,-538 118.5,-532 118.5,-532 118.5,-473 118.5,-473 118.5,-467 124.5,-461 130.5,-461 130.5,-461 252.5,-461 252.5,-461 258.5,-461 264.5,-467 264.5,-473 264.5,-473 264.5,-532 264.5,-532 264.5,-538 258.5,-544 252.5,-544\"/>\n<text text-anchor=\"start\" x=\"126.5\" y=\"-528.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ≤ 1.6</text>\n<text text-anchor=\"start\" x=\"156\" y=\"-513.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.059</text>\n<text text-anchor=\"start\" x=\"150.5\" y=\"-498.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 33</text>\n<text text-anchor=\"start\" x=\"141\" y=\"-483.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 32, 1]</text>\n<text text-anchor=\"start\" x=\"139\" y=\"-468.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 2&#45;&gt;3 -->\n<g id=\"edge3\" class=\"edge\">\n<title>2&#45;&gt;3</title>\n<path fill=\"none\" stroke=\"black\" d=\"M246.36,-579.91C239.97,-571.01 233.15,-561.51 226.56,-552.33\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"229.27,-550.1 220.59,-544.02 223.58,-554.19 229.27,-550.1\"/>\n</g>\n<!-- 6 -->\n<g id=\"node7\" class=\"node\">\n<title>6</title>\n<path fill=\"#9254e9\" stroke=\"black\" d=\"M424.5,-544C424.5,-544 294.5,-544 294.5,-544 288.5,-544 282.5,-538 282.5,-532 282.5,-532 282.5,-473 282.5,-473 282.5,-467 288.5,-461 294.5,-461 294.5,-461 424.5,-461 424.5,-461 430.5,-461 436.5,-467 436.5,-473 436.5,-473 436.5,-532 436.5,-532 436.5,-538 430.5,-544 424.5,-544\"/>\n<text text-anchor=\"start\" x=\"290.5\" y=\"-528.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ≤ 1.75</text>\n<text text-anchor=\"start\" x=\"324\" y=\"-513.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.214</text>\n<text text-anchor=\"start\" x=\"318.5\" y=\"-498.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 41</text>\n<text text-anchor=\"start\" x=\"309\" y=\"-483.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 5, 36]</text>\n<text text-anchor=\"start\" x=\"311\" y=\"-468.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 2&#45;&gt;6 -->\n<g id=\"edge6\" class=\"edge\">\n<title>2&#45;&gt;6</title>\n<path fill=\"none\" stroke=\"black\" d=\"M304.64,-579.91C311.03,-571.01 317.85,-561.51 324.44,-552.33\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"327.42,-554.19 330.41,-544.02 321.73,-550.1 327.42,-554.19\"/>\n</g>\n<!-- 4 -->\n<g id=\"node5\" class=\"node\">\n<title>4</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M109,-417.5C109,-417.5 12,-417.5 12,-417.5 6,-417.5 0,-411.5 0,-405.5 0,-405.5 0,-361.5 0,-361.5 0,-355.5 6,-349.5 12,-349.5 12,-349.5 109,-349.5 109,-349.5 115,-349.5 121,-355.5 121,-361.5 121,-361.5 121,-405.5 121,-405.5 121,-411.5 115,-417.5 109,-417.5\"/>\n<text text-anchor=\"start\" x=\"32.5\" y=\"-402.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"19.5\" y=\"-387.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 32</text>\n<text text-anchor=\"start\" x=\"10\" y=\"-372.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 32, 0]</text>\n<text text-anchor=\"start\" x=\"8\" y=\"-357.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 3&#45;&gt;4 -->\n<g id=\"edge4\" class=\"edge\">\n<title>3&#45;&gt;4</title>\n<path fill=\"none\" stroke=\"black\" d=\"M146.05,-460.91C132.83,-449.1 118.4,-436.22 105.23,-424.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"107.43,-421.72 97.64,-417.67 102.76,-426.94 107.43,-421.72\"/>\n</g>\n<!-- 5 -->\n<g id=\"node6\" class=\"node\">\n<title>5</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M240,-417.5C240,-417.5 151,-417.5 151,-417.5 145,-417.5 139,-411.5 139,-405.5 139,-405.5 139,-361.5 139,-361.5 139,-355.5 145,-349.5 151,-349.5 151,-349.5 240,-349.5 240,-349.5 246,-349.5 252,-355.5 252,-361.5 252,-361.5 252,-405.5 252,-405.5 252,-411.5 246,-417.5 240,-417.5\"/>\n<text text-anchor=\"start\" x=\"167.5\" y=\"-402.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"158\" y=\"-387.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n<text text-anchor=\"start\" x=\"148.5\" y=\"-372.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 1]</text>\n<text text-anchor=\"start\" x=\"147\" y=\"-357.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 3&#45;&gt;5 -->\n<g id=\"edge5\" class=\"edge\">\n<title>3&#45;&gt;5</title>\n<path fill=\"none\" stroke=\"black\" d=\"M192.89,-460.91C193.25,-450.2 193.65,-438.62 194.02,-427.78\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"197.52,-427.78 194.37,-417.67 190.53,-427.54 197.52,-427.78\"/>\n</g>\n<!-- 7 -->\n<g id=\"node8\" class=\"node\">\n<title>7</title>\n<path fill=\"#ffffff\" stroke=\"black\" d=\"M424,-425C424,-425 289,-425 289,-425 283,-425 277,-419 277,-413 277,-413 277,-354 277,-354 277,-348 283,-342 289,-342 289,-342 424,-342 424,-342 430,-342 436,-348 436,-354 436,-354 436,-413 436,-413 436,-419 430,-425 424,-425\"/>\n<text text-anchor=\"start\" x=\"285\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 4.95</text>\n<text text-anchor=\"start\" x=\"328.5\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\n<text text-anchor=\"start\" x=\"319\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 8</text>\n<text text-anchor=\"start\" x=\"309.5\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 4, 4]</text>\n<text text-anchor=\"start\" x=\"304\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 6&#45;&gt;7 -->\n<g id=\"edge7\" class=\"edge\">\n<title>6&#45;&gt;7</title>\n<path fill=\"none\" stroke=\"black\" d=\"M358.46,-460.91C358.25,-452.56 358.02,-443.67 357.8,-435.02\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"361.29,-434.93 357.54,-425.02 354.3,-435.11 361.29,-434.93\"/>\n</g>\n<!-- 14 -->\n<g id=\"node15\" class=\"node\">\n<title>14</title>\n<path fill=\"#853fe6\" stroke=\"black\" d=\"M601,-425C601,-425 466,-425 466,-425 460,-425 454,-419 454,-413 454,-413 454,-354 454,-354 454,-348 460,-342 466,-342 466,-342 601,-342 601,-342 607,-342 613,-348 613,-354 613,-354 613,-413 613,-413 613,-419 607,-425 601,-425\"/>\n<text text-anchor=\"start\" x=\"462\" y=\"-409.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 4.85</text>\n<text text-anchor=\"start\" x=\"498\" y=\"-394.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.059</text>\n<text text-anchor=\"start\" x=\"492.5\" y=\"-379.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 33</text>\n<text text-anchor=\"start\" x=\"483\" y=\"-364.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 32]</text>\n<text text-anchor=\"start\" x=\"485\" y=\"-349.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 6&#45;&gt;14 -->\n<g id=\"edge14\" class=\"edge\">\n<title>6&#45;&gt;14</title>\n<path fill=\"none\" stroke=\"black\" d=\"M419.87,-460.91C434.31,-451.2 449.83,-440.76 464.63,-430.81\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"466.89,-433.51 473.24,-425.02 462.99,-427.7 466.89,-433.51\"/>\n</g>\n<!-- 8 -->\n<g id=\"node9\" class=\"node\">\n<title>8</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M252,-298.5C252,-298.5 155,-298.5 155,-298.5 149,-298.5 143,-292.5 143,-286.5 143,-286.5 143,-242.5 143,-242.5 143,-236.5 149,-230.5 155,-230.5 155,-230.5 252,-230.5 252,-230.5 258,-230.5 264,-236.5 264,-242.5 264,-242.5 264,-286.5 264,-286.5 264,-292.5 258,-298.5 252,-298.5\"/>\n<text text-anchor=\"start\" x=\"175.5\" y=\"-283.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"166\" y=\"-268.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\n<text text-anchor=\"start\" x=\"156.5\" y=\"-253.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 0]</text>\n<text text-anchor=\"start\" x=\"151\" y=\"-238.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 7&#45;&gt;8 -->\n<g id=\"edge8\" class=\"edge\">\n<title>7&#45;&gt;8</title>\n<path fill=\"none\" stroke=\"black\" d=\"M303.42,-341.91C287.69,-329.88 270.5,-316.73 254.88,-304.79\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"256.94,-301.96 246.87,-298.67 252.69,-307.52 256.94,-301.96\"/>\n</g>\n<!-- 9 -->\n<g id=\"node10\" class=\"node\">\n<title>9</title>\n<path fill=\"#c09cf2\" stroke=\"black\" d=\"M424.5,-306C424.5,-306 294.5,-306 294.5,-306 288.5,-306 282.5,-300 282.5,-294 282.5,-294 282.5,-235 282.5,-235 282.5,-229 288.5,-223 294.5,-223 294.5,-223 424.5,-223 424.5,-223 430.5,-223 436.5,-229 436.5,-235 436.5,-235 436.5,-294 436.5,-294 436.5,-300 430.5,-306 424.5,-306\"/>\n<text text-anchor=\"start\" x=\"290.5\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ≤ 1.55</text>\n<text text-anchor=\"start\" x=\"324\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n<text text-anchor=\"start\" x=\"322\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 6</text>\n<text text-anchor=\"start\" x=\"312.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 4]</text>\n<text text-anchor=\"start\" x=\"311\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 7&#45;&gt;9 -->\n<g id=\"edge9\" class=\"edge\">\n<title>7&#45;&gt;9</title>\n<path fill=\"none\" stroke=\"black\" d=\"M357.54,-341.91C357.75,-333.56 357.98,-324.67 358.2,-316.02\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"361.7,-316.11 358.46,-306.02 354.71,-315.93 361.7,-316.11\"/>\n</g>\n<!-- 10 -->\n<g id=\"node11\" class=\"node\">\n<title>10</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M254,-179.5C254,-179.5 165,-179.5 165,-179.5 159,-179.5 153,-173.5 153,-167.5 153,-167.5 153,-123.5 153,-123.5 153,-117.5 159,-111.5 165,-111.5 165,-111.5 254,-111.5 254,-111.5 260,-111.5 266,-117.5 266,-123.5 266,-123.5 266,-167.5 266,-167.5 266,-173.5 260,-179.5 254,-179.5\"/>\n<text text-anchor=\"start\" x=\"181.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"172\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n<text text-anchor=\"start\" x=\"162.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 3]</text>\n<text text-anchor=\"start\" x=\"161\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 9&#45;&gt;10 -->\n<g id=\"edge10\" class=\"edge\">\n<title>9&#45;&gt;10</title>\n<path fill=\"none\" stroke=\"black\" d=\"M307.46,-222.91C292.18,-210.99 275.49,-197.98 260.29,-186.12\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"262.06,-183.06 252.02,-179.67 257.75,-188.58 262.06,-183.06\"/>\n</g>\n<!-- 11 -->\n<g id=\"node12\" class=\"node\">\n<title>11</title>\n<path fill=\"#9cf2c0\" stroke=\"black\" d=\"M431,-187C431,-187 296,-187 296,-187 290,-187 284,-181 284,-175 284,-175 284,-116 284,-116 284,-110 290,-104 296,-104 296,-104 431,-104 431,-104 437,-104 443,-110 443,-116 443,-116 443,-175 443,-175 443,-181 437,-187 431,-187\"/>\n<text text-anchor=\"start\" x=\"292\" y=\"-171.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 5.45</text>\n<text text-anchor=\"start\" x=\"328\" y=\"-156.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n<text text-anchor=\"start\" x=\"326\" y=\"-141.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n<text text-anchor=\"start\" x=\"316.5\" y=\"-126.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 1]</text>\n<text text-anchor=\"start\" x=\"311\" y=\"-111.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 9&#45;&gt;11 -->\n<g id=\"edge11\" class=\"edge\">\n<title>9&#45;&gt;11</title>\n<path fill=\"none\" stroke=\"black\" d=\"M360.89,-222.91C361.17,-214.56 361.48,-205.67 361.77,-197.02\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"365.27,-197.13 362.11,-187.02 358.27,-196.9 365.27,-197.13\"/>\n</g>\n<!-- 12 -->\n<g id=\"node13\" class=\"node\">\n<title>12</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M345,-68C345,-68 248,-68 248,-68 242,-68 236,-62 236,-56 236,-56 236,-12 236,-12 236,-6 242,0 248,0 248,0 345,0 345,0 351,0 357,-6 357,-12 357,-12 357,-56 357,-56 357,-62 351,-68 345,-68\"/>\n<text text-anchor=\"start\" x=\"268.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"259\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\n<text text-anchor=\"start\" x=\"249.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 0]</text>\n<text text-anchor=\"start\" x=\"244\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 11&#45;&gt;12 -->\n<g id=\"edge12\" class=\"edge\">\n<title>11&#45;&gt;12</title>\n<path fill=\"none\" stroke=\"black\" d=\"M338.55,-103.73C333.19,-94.97 327.52,-85.7 322.14,-76.91\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"325.08,-75 316.88,-68.3 319.11,-78.66 325.08,-75\"/>\n</g>\n<!-- 13 -->\n<g id=\"node14\" class=\"node\">\n<title>13</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M476,-68C476,-68 387,-68 387,-68 381,-68 375,-62 375,-56 375,-56 375,-12 375,-12 375,-6 381,0 387,0 387,0 476,0 476,0 482,0 488,-6 488,-12 488,-12 488,-56 488,-56 488,-62 482,-68 476,-68\"/>\n<text text-anchor=\"start\" x=\"403.5\" y=\"-52.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"394\" y=\"-37.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n<text text-anchor=\"start\" x=\"384.5\" y=\"-22.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 1]</text>\n<text text-anchor=\"start\" x=\"383\" y=\"-7.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 11&#45;&gt;13 -->\n<g id=\"edge13\" class=\"edge\">\n<title>11&#45;&gt;13</title>\n<path fill=\"none\" stroke=\"black\" d=\"M388.82,-103.73C394.26,-94.97 400.01,-85.7 405.48,-76.91\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"408.52,-78.64 410.82,-68.3 402.57,-74.95 408.52,-78.64\"/>\n</g>\n<!-- 15 -->\n<g id=\"node16\" class=\"node\">\n<title>15</title>\n<path fill=\"#c09cf2\" stroke=\"black\" d=\"M594,-306C594,-306 469,-306 469,-306 463,-306 457,-300 457,-294 457,-294 457,-235 457,-235 457,-229 463,-223 469,-223 469,-223 594,-223 594,-223 600,-223 606,-229 606,-235 606,-235 606,-294 606,-294 606,-300 600,-306 594,-306\"/>\n<text text-anchor=\"start\" x=\"465\" y=\"-290.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">sepal width (cm) ≤ 3.1</text>\n<text text-anchor=\"start\" x=\"496\" y=\"-275.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n<text text-anchor=\"start\" x=\"494\" y=\"-260.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n<text text-anchor=\"start\" x=\"484.5\" y=\"-245.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 2]</text>\n<text text-anchor=\"start\" x=\"483\" y=\"-230.8\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 14&#45;&gt;15 -->\n<g id=\"edge15\" class=\"edge\">\n<title>14&#45;&gt;15</title>\n<path fill=\"none\" stroke=\"black\" d=\"M532.81,-341.91C532.66,-333.56 532.51,-324.67 532.36,-316.02\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"535.86,-315.96 532.19,-306.02 528.86,-316.08 535.86,-315.96\"/>\n</g>\n<!-- 18 -->\n<g id=\"node19\" class=\"node\">\n<title>18</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M729,-298.5C729,-298.5 636,-298.5 636,-298.5 630,-298.5 624,-292.5 624,-286.5 624,-286.5 624,-242.5 624,-242.5 624,-236.5 630,-230.5 636,-230.5 636,-230.5 729,-230.5 729,-230.5 735,-230.5 741,-236.5 741,-242.5 741,-242.5 741,-286.5 741,-286.5 741,-292.5 735,-298.5 729,-298.5\"/>\n<text text-anchor=\"start\" x=\"654.5\" y=\"-283.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"641.5\" y=\"-268.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 30</text>\n<text text-anchor=\"start\" x=\"632\" y=\"-253.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 30]</text>\n<text text-anchor=\"start\" x=\"634\" y=\"-238.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 14&#45;&gt;18 -->\n<g id=\"edge18\" class=\"edge\">\n<title>14&#45;&gt;18</title>\n<path fill=\"none\" stroke=\"black\" d=\"M585.19,-341.91C600.37,-329.99 616.95,-316.98 632.04,-305.12\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"634.56,-307.6 640.26,-298.67 630.24,-302.09 634.56,-307.6\"/>\n</g>\n<!-- 16 -->\n<g id=\"node17\" class=\"node\">\n<title>16</title>\n<path fill=\"#8139e5\" stroke=\"black\" d=\"M573,-179.5C573,-179.5 484,-179.5 484,-179.5 478,-179.5 472,-173.5 472,-167.5 472,-167.5 472,-123.5 472,-123.5 472,-117.5 478,-111.5 484,-111.5 484,-111.5 573,-111.5 573,-111.5 579,-111.5 585,-117.5 585,-123.5 585,-123.5 585,-167.5 585,-167.5 585,-173.5 579,-179.5 573,-179.5\"/>\n<text text-anchor=\"start\" x=\"500.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"491\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 2</text>\n<text text-anchor=\"start\" x=\"481.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 2]</text>\n<text text-anchor=\"start\" x=\"480\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n</g>\n<!-- 15&#45;&gt;16 -->\n<g id=\"edge16\" class=\"edge\">\n<title>15&#45;&gt;16</title>\n<path fill=\"none\" stroke=\"black\" d=\"M530.46,-222.91C530.18,-212.2 529.89,-200.62 529.61,-189.78\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"533.11,-189.57 529.35,-179.67 526.11,-189.75 533.11,-189.57\"/>\n</g>\n<!-- 17 -->\n<g id=\"node18\" class=\"node\">\n<title>17</title>\n<path fill=\"#39e581\" stroke=\"black\" d=\"M712,-179.5C712,-179.5 615,-179.5 615,-179.5 609,-179.5 603,-173.5 603,-167.5 603,-167.5 603,-123.5 603,-123.5 603,-117.5 609,-111.5 615,-111.5 615,-111.5 712,-111.5 712,-111.5 718,-111.5 724,-117.5 724,-123.5 724,-123.5 724,-167.5 724,-167.5 724,-173.5 718,-179.5 712,-179.5\"/>\n<text text-anchor=\"start\" x=\"635.5\" y=\"-164.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n<text text-anchor=\"start\" x=\"626\" y=\"-149.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 1</text>\n<text text-anchor=\"start\" x=\"616.5\" y=\"-134.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 0]</text>\n<text text-anchor=\"start\" x=\"611\" y=\"-119.3\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n</g>\n<!-- 15&#45;&gt;17 -->\n<g id=\"edge17\" class=\"edge\">\n<title>15&#45;&gt;17</title>\n<path fill=\"none\" stroke=\"black\" d=\"M577.3,-222.91C590.62,-211.1 605.15,-198.22 618.43,-186.45\"/>\n<polygon fill=\"black\" stroke=\"black\" points=\"620.92,-188.92 626.08,-179.67 616.28,-183.68 620.92,-188.92\"/>\n</g>\n</g>\n</svg>\n",
            "text/plain": [
              "<graphviz.sources.Source at 0x7b1fa33ba8d0>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. **Write a Python program to train a Decision Tree Classifier with a maximum depth of 3 and compare its accuracy with a fully grown tree.**"
      ],
      "metadata": {
        "id": "lYr_XemUbscm"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4754b51e",
        "outputId": "3642ffe7-6b55-4239-e5be-52535b47888f"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a fully grown Decision Tree Classifier\n",
        "clf_full = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the fully grown classifier\n",
        "clf_full.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and calculate accuracy for the fully grown tree\n",
        "y_pred_full = clf_full.predict(X_test)\n",
        "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
        "print(f\"Accuracy of the fully grown Decision Tree Classifier: {accuracy_full:.2f}\")\n",
        "\n",
        "# Create a Decision Tree Classifier with max_depth=3\n",
        "clf_pruned = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "\n",
        "# Train the pruned classifier\n",
        "clf_pruned.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and calculate accuracy for the pruned tree\n",
        "y_pred_pruned = clf_pruned.predict(X_test)\n",
        "accuracy_pruned = accuracy_score(y_test, y_pred_pruned)\n",
        "print(f\"Accuracy of the Decision Tree Classifier with max_depth=3: {accuracy_pruned:.2f}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the fully grown Decision Tree Classifier: 1.00\n",
            "Accuracy of the Decision Tree Classifier with max_depth=3: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. **Write a Python program to train a Decision Tree Classifier using min_samples_split=5 and compare its accuracy with a default tree.**"
      ],
      "metadata": {
        "id": "1EioZDpkb3Ft"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GqhuBImFb5x8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b5b7a90",
        "outputId": "2d657610-fc8b-49bb-bf10-9e083b178da9"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a default Decision Tree Classifier\n",
        "clf_default = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the default classifier\n",
        "clf_default.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and calculate accuracy for the default tree\n",
        "y_pred_default = clf_default.predict(X_test)\n",
        "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
        "print(f\"Accuracy of the default Decision Tree Classifier: {accuracy_default:.2f}\")\n",
        "\n",
        "# Create a Decision Tree Classifier with min_samples_split=5\n",
        "clf_split5 = DecisionTreeClassifier(min_samples_split=5, random_state=42)\n",
        "\n",
        "# Train the classifier with min_samples_split=5\n",
        "clf_split5.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and calculate accuracy for the tree with min_samples_split=5\n",
        "y_pred_split5 = clf_split5.predict(X_test)\n",
        "accuracy_split5 = accuracy_score(y_test, y_pred_split5)\n",
        "print(f\"Accuracy of the Decision Tree Classifier with min_samples_split=5: {accuracy_split5:.2f}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the default Decision Tree Classifier: 1.00\n",
            "Accuracy of the Decision Tree Classifier with min_samples_split=5: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. **Write a Python program to apply feature scaling before training a Decision Tree Classifier and compare its accuracy with unscaled data.**"
      ],
      "metadata": {
        "id": "9sWpuVgyb_GI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9385ad3",
        "outputId": "545d6dd2-0504-43eb-d4f7-63063c750466"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets (unscaled)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create and train a Decision Tree Classifier on unscaled data\n",
        "clf_unscaled = DecisionTreeClassifier(random_state=42)\n",
        "clf_unscaled.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and calculate accuracy for unscaled data\n",
        "y_pred_unscaled = clf_unscaled.predict(X_test)\n",
        "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
        "print(f\"Accuracy of the Decision Tree Classifier on unscaled data: {accuracy_unscaled:.2f}\")\n",
        "\n",
        "# Apply feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Create and train a Decision Tree Classifier on scaled data\n",
        "clf_scaled = DecisionTreeClassifier(random_state=42)\n",
        "clf_scaled.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Make predictions and calculate accuracy for scaled data\n",
        "y_pred_scaled = clf_scaled.predict(X_test_scaled)\n",
        "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "print(f\"Accuracy of the Decision Tree Classifier on scaled data: {accuracy_scaled:.2f}\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the Decision Tree Classifier on unscaled data: 1.00\n",
            "Accuracy of the Decision Tree Classifier on scaled data: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. **Write a Python program to train a Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass classification.**"
      ],
      "metadata": {
        "id": "bzj3zAzWcH7Q"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98fdc42a",
        "outputId": "2d88613a-e652-48a8-b1c5-a3ec2623de46"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a base Decision Tree Classifier\n",
        "base_clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Create a One-vs-Rest Classifier using the base Decision Tree Classifier\n",
        "ovr_clf = OneVsRestClassifier(base_clf)\n",
        "\n",
        "# Train the OvR classifier\n",
        "ovr_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_ovr = ovr_clf.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy_ovr = accuracy_score(y_test, y_pred_ovr)\n",
        "print(f\"Accuracy of the Decision Tree Classifier with One-vs-Rest strategy: {accuracy_ovr:.2f}\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the Decision Tree Classifier with One-vs-Rest strategy: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. **Write a Python program to train a Decision Tree Classifier and display the feature importance scores.**"
      ],
      "metadata": {
        "id": "QXf9X_gncOZD"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59f42a08",
        "outputId": "ee170b3c-4f77-487b-d50d-37c9df3a6f45"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets (optional)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = clf.feature_importances_\n",
        "\n",
        "# Display feature importances\n",
        "print(\"Feature Importances:\")\n",
        "for i, importance in enumerate(feature_importances):\n",
        "    print(f\"{iris.feature_names[i]}: {importance:.4f}\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances:\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0191\n",
            "petal length (cm): 0.8933\n",
            "petal width (cm): 0.0876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. **Write a Python program to train a Decision Tree Regressor with max_depth=5 and compare its performance with an unrestricted tree.**"
      ],
      "metadata": {
        "id": "_kOeIOqMcVmj"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68e4c150",
        "outputId": "ef8a63b8-400e-4c2a-a71a-ad7d6abf1817"
      },
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load the California housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create an unrestricted Decision Tree Regressor\n",
        "regressor_full = DecisionTreeRegressor(random_state=42)\n",
        "\n",
        "# Train the unrestricted regressor\n",
        "regressor_full.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and calculate MSE for the unrestricted tree\n",
        "y_pred_full = regressor_full.predict(X_test)\n",
        "mse_full = mean_squared_error(y_test, y_pred_full)\n",
        "print(f\"Mean Squared Error (MSE) of the unrestricted Decision Tree Regressor: {mse_full:.2f}\")\n",
        "\n",
        "# Create a Decision Tree Regressor with max_depth=5\n",
        "regressor_pruned = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
        "\n",
        "# Train the pruned regressor\n",
        "regressor_pruned.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions and calculate MSE for the pruned tree\n",
        "y_pred_pruned = regressor_pruned.predict(X_test)\n",
        "mse_pruned = mean_squared_error(y_test, y_pred_pruned)\n",
        "print(f\"Mean Squared Error (MSE) of the Decision Tree Regressor with max_depth=5: {mse_pruned:.2f}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE) of the unrestricted Decision Tree Regressor: 0.53\n",
            "Mean Squared Error (MSE) of the Decision Tree Regressor with max_depth=5: 0.52\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. **Write a Python program to train a Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and visualize its effect on accuracy.**"
      ],
      "metadata": {
        "id": "z2P-DHHvccU7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "f54a7b63",
        "outputId": "f3d63257-7aff-4f58-cd3d-7486fd6f515d"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load and Split Data\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 2. Train a Decision Tree\n",
        "# Train a decision tree to find the cost complexity pruning path\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# 3. Calculate Cost Complexity Path\n",
        "path = clf.cost_complexity_pruning_path(X_train, y_train)\n",
        "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
        "\n",
        "# For each alpha, we will train a new tree\n",
        "clfs = []\n",
        "for ccp_alpha in ccp_alphas:\n",
        "    clf = DecisionTreeClassifier(random_state=42, ccp_alpha=ccp_alpha)\n",
        "    clf.fit(X_train, y_train)\n",
        "    clfs.append(clf)\n",
        "\n",
        "# Remove the last element in clfs and ccp_alphas, because it is the trivial tree with only one node\n",
        "clfs = clfs[:-1]\n",
        "ccp_alphas = ccp_alphas[:-1]\n",
        "\n",
        "# 4. Train Trees for Each Alpha (Done in the loop above)\n",
        "\n",
        "# 5. Calculate Accuracy for Each Tree\n",
        "train_scores = [clf.score(X_train, y_train) for clf in clfs]\n",
        "test_scores = [clf.score(X_test, y_test) for clf in clfs]\n",
        "\n",
        "# 6. Visualize Accuracy vs. Alpha\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_xlabel(\"alpha\")\n",
        "ax.set_ylabel(\"accuracy\")\n",
        "ax.set_title(\"Accuracy vs. alpha for training and testing sets\")\n",
        "ax.plot(ccp_alphas, train_scores, marker='o', label=\"train\",\n",
        "        drawstyle=\"steps-post\")\n",
        "ax.plot(ccp_alphas, test_scores, marker='o', label=\"test\",\n",
        "        drawstyle=\"steps-post\")\n",
        "ax.legend()\n",
        "plt.show()\n",
        "\n",
        "# Optional: Find the alpha that maximizes test accuracy and evaluate the corresponding tree\n",
        "optimal_alpha_index = test_scores.index(max(test_scores))\n",
        "optimal_alpha = ccp_alphas[optimal_alpha_index]\n",
        "print(f\"\\nOptimal alpha based on test accuracy: {optimal_alpha:.4f}\")\n",
        "\n",
        "optimal_clf = DecisionTreeClassifier(random_state=42, ccp_alpha=optimal_alpha)\n",
        "optimal_clf.fit(X_train, y_train)\n",
        "optimal_test_accuracy = accuracy_score(y_test, optimal_clf.predict(X_test))\n",
        "print(f\"Accuracy of the Decision Tree with optimal alpha on the test set: {optimal_test_accuracy:.2f}\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Optimal alpha based on test accuracy: 0.0000\n",
            "Accuracy of the Decision Tree with optimal alpha on the test set: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. **Write a Python program to train a Decision Tree Classifier and evaluate its performance using Precision, Recall, and F1-Score.**"
      ],
      "metadata": {
        "id": "ienZWh7uc3jk"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbcecc2b",
        "outputId": "21eb508e-b2fa-4048-a97d-23f7a933aad0"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate and print Accuracy, Precision, Recall, and F1-Score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "# For multiclass classification, specify average='weighted' or 'macro'\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.00\n",
            "Precision: 1.00\n",
            "Recall: 1.00\n",
            "F1-Score: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. **Write a Python program to train a Decision Tree Classifier and visualize the confusion matrix using seaborn.**"
      ],
      "metadata": {
        "id": "Kjq73_s2dHVu"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "9f7d101a",
        "outputId": "b6149f1e-6096-4e8b-d70b-9b12a55a1a1e"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Train the classifier\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = clf.predict(X_test)\n",
        "\n",
        "# Calculate the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Visualize the confusion matrix using seaborn\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Optional: Print accuracy as well\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the Decision Tree Classifier: {accuracy:.2f}\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the Decision Tree Classifier: 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. **Write a Python program to train a Decision Tree Classifier and use GridSearchCV to find the optimal values for max_depth and min_samples_split.**"
      ],
      "metadata": {
        "id": "qXQFVy5bdOqK"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45741292",
        "outputId": "06beab5d-a0b9-497f-c4ca-aa2d59ad2c7c"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [None, 2, 3, 4, 5, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10, 20]\n",
        "}\n",
        "\n",
        "# Create GridSearchCV object\n",
        "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit GridSearchCV to the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and best score\n",
        "print(\"Best parameters found by GridSearchCV:\")\n",
        "print(grid_search.best_params_)\n",
        "print(f\"\\nBest cross-validation accuracy: {grid_search.best_score_:.2f}\")\n",
        "\n",
        "# Optional: Evaluate the best model on the test set\n",
        "best_clf = grid_search.best_estimator_\n",
        "test_accuracy = best_clf.score(X_test, y_test)\n",
        "print(f\"\\nAccuracy of the best model on the test set: {test_accuracy:.2f}\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found by GridSearchCV:\n",
            "{'max_depth': None, 'min_samples_split': 10}\n",
            "\n",
            "Best cross-validation accuracy: 0.94\n",
            "\n",
            "Accuracy of the best model on the test set: 1.00\n"
          ]
        }
      ]
    }
  ]
}