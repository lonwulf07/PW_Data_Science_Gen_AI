{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Regression**\n"
      ],
      "metadata": {
        "id": "8snOsPq2gF6Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **What is Simple Linear Regression?**\n",
        "\n",
        "  Ans. Simple Linear Regression is a statistical method used to model the relationship between two continuous variables:\n",
        "\n",
        "  - One variable is considered to be the independent variable (or predictor variable), often denoted as X.\n",
        "  - The other variable is considered to be the dependent variable (or response variable), often denoted as Y.\n",
        "\n",
        "  The goal of simple linear regression is to find a linear equation that best describes how Y changes as X changes. This relationship is represented by a straight line.\n",
        "---"
      ],
      "metadata": {
        "id": "lKVOqcpvgIGU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. **What are the key assumptions of Simple Linear Regression?**\n",
        "\n",
        "  Ans. The key assumptions are:\n",
        "\n",
        "  - Linearity: The relationship between the independent variable (X) and the dependent variable (Y) is linear. If this isn't the case, a linear model won't accurately capture the relationship, and we might need to consider non-linear transformations of our features or the target.\n",
        "\n",
        "  - Independence of Errors: The residuals (the differences between the observed and predicted values) are independent of each other. This means that the error for one data point should not influence the error for another. Violations, like autocorrelation in time series data, can lead to inefficient parameter estimates.\n",
        "\n",
        "  - Homoscedasticity: The variance of the residuals is constant across all levels of the independent variable (X). If the variance of the errors differs (heteroscedasticity), the precision of the model's predictions will vary across the range of X. This might suggest that certain ranges of our feature are less reliable for prediction.\n",
        "\n",
        "  - Normality of Residuals: The residuals are approximately normally distributed. This assumption is primarily important for hypothesis testing and the construction of confidence intervals. While the Central Limit Theorem can mitigate this concern with large datasets, significant deviations from normality in smaller datasets can affect the validity of statistical inferences about the model parameters.\n",
        "---"
      ],
      "metadata": {
        "id": "pkIpL8A1gxEx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. **What does the coefficient m represent in the equation Y=mX+c?**\n",
        "\n",
        "  Ans. The coefficient m represents the slope of the line.\n",
        "\n",
        "  Specifically, m tells us:\n",
        "\n",
        "  - The rate of change of Y with respect to X: For every one-unit increase in X, Y is expected to change by m units.\n",
        "  - The direction of the relationship:\n",
        "    - If m>0, there is a positive linear relationship (as X increases, Y tends to increase).\n",
        "    - If m<0, there is a negative linear relationship (as X increases, Y tends to decrease).\n",
        "    - If m=0, there is no linear relationship (the line is horizontal).\n",
        "---"
      ],
      "metadata": {
        "id": "1ql1yRuolq_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. **What does the intercept c represent in the equation Y=mX+c?**\n",
        "\n",
        "  Ans. The intercept c in the equation Y=mX+c represents the value of Y when X is equal to zero. It's the point where the regression line crosses the y-axis.\n",
        "---"
      ],
      "metadata": {
        "id": "BY1U3L6KlvuZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. **How do we calculate the slope m in Simple Linear Regression?**\n",
        "\n",
        "  Ans. In Simple Linear Regression, the slope m is calculated using the data points from our dataset.\n",
        "\n",
        "  The most common way to calculate the slope is using the formula derived from the Ordinary Least Squares (OLS) method, which aims to minimize the sum of the squared differences between the observed y values and the values predicted by the linear model."
      ],
      "metadata": {
        "id": "dOwf1w1Jl1DA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. **What is the purpose of the least squares method in Simple Linear Regression?**\n",
        "\n",
        "  Ans. The primary purpose of the least squares method in Simple Linear Regression is to find the \"best-fitting\" line through a set of data points. It achieves this by minimizing the sum of the squared differences between the observed values of the dependent variable and the values predicted by the linear model.\n",
        "---"
      ],
      "metadata": {
        "id": "Zy0WRHy_l3gq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. **How is the coefficient of determination (R²) interpreted in Simple Linear Regression?**\n",
        "\n",
        "  Ans. The coefficient of determination, R², is a crucial metric for evaluating how well a simple linear regression model fits the observed data.\n",
        "\n",
        "  R² represents the proportion of the variance in the dependent variable (Y) that is predictable from the independent variable (X). It essentially tells us how much of the variation in the target variable can be explained by the linear relationship with our feature.\n",
        "---"
      ],
      "metadata": {
        "id": "4eLTOgRRl5T3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. **What is Multiple Linear Regression?**\n",
        "\n",
        "  Ans. Multiple Linear Regression is a statistical technique that extends Simple Linear Regression to model the linear relationship between a dependent variable and two or more independent variables.\n",
        "---"
      ],
      "metadata": {
        "id": "mg4BhF1Ul6eB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. **What is the main difference between Simple and Multiple Linear Regression?**\n",
        "\n",
        "  Ans. The key difference between Simple and Multiple Linear Regression lies in the number of independent variables used to predict the dependent variable.\n",
        "\n",
        "  - Simple Linear Regression: Uses one independent variable (X) to model the linear relationship with a dependent variable (Y).\n",
        "\n",
        "  - Multiple Linear Regression: Uses two or more independent variables (X1, X2,..., Xp) to model the linear relationship with a dependent variable (Y).\n",
        "---"
      ],
      "metadata": {
        "id": "QpSq2Qfgl7Xo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. **What are the key assumptions of Multiple Linear Regression?**\n",
        "\n",
        "  Ans. The key assumptions are:\n",
        "\n",
        "  - Linearity: There is a linear relationship between each of the independent variables and the dependent variable. You can check this by examining scatter plots of each independent variable against the dependent variable.\n",
        "\n",
        "  - Independence of Errors: The residuals (the differences between the observed and predicted values) are independent of each other. This means that the error for one data point should not influence the error for another. This is often checked by looking at plots of residuals over time (if the data has a time component) or by using statistical tests like the Durbin-Watson test.\n",
        "\n",
        "  - Homoscedasticity: The variance of the residuals is constant across all levels of the independent variables. You can check this by plotting the residuals against the predicted values. The spread of the residuals should be roughly constant. A funnel shape, for example, indicates heteroscedasticity.\n",
        "\n",
        "  - Normality of Residuals: The residuals are approximately normally distributed. This is important for hypothesis testing and confidence intervals. You can check this using histograms, Q-Q plots, or statistical tests like the Shapiro-Wilk test on the residuals.\n",
        "\n",
        "  - No Multicollinearity: The independent variables should not be highly correlated with each other. High multicollinearity can make it difficult to interpret the individual coefficients and can inflate their standard errors. You can check for this using correlation matrices or by calculating the Variance Inflation Factor (VIF) for each independent variable. A high VIF (e.g., > 10) often indicates a problem.\n",
        "---"
      ],
      "metadata": {
        "id": "o1OR0bKPl8Ty"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. **What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?**\n",
        "\n",
        "  Ans. Heteroscedasticity refers to a situation where the variance of the error terms (residuals) in a regression model is not constant across all levels of the independent variables. In simpler terms, the spread of the residuals differs as the values of the predictor variables change. The opposite of heteroscedasticity is homoscedasticity, where the variance of the errors is constant.\n",
        "\n",
        "  Heteroscedasticity affect the results of a Multiple Linear Regression model in the following ways:\n",
        "\n",
        "  - Inefficient Coefficient Estimates: While Ordinary Least Squares (OLS) estimators remain unbiased in the presence of heteroscedasticity, they are no longer the Best Linear Unbiased Estimators (BLUE). This means that there might be other unbiased estimators with smaller variance.\n",
        "\n",
        "  - Unreliable Standard Errors: The standard errors of the estimated regression coefficients, which are used for hypothesis testing and constructing confidence intervals, become biased. Typically, OLS underestimates the standard errors when heteroscedasticity is present.\n",
        "\n",
        "  - Invalid Statistical Inferences: Because the standard errors are unreliable, the t-statistics and F-statistics used for hypothesis testing are also affected, leading to potentially incorrect conclusions about the significance of the independent variables. You might falsely conclude that a feature is statistically significant when it is not, or vice versa.\n",
        "\n",
        "  - Suboptimal Predictions: Although the coefficient estimates are still unbiased, the predictions made by the model might not be as precise as they could be if the model accounted for the varying error variance.\n",
        "---\n"
      ],
      "metadata": {
        "id": "n73KDHmUl9EJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. **How can you improve a Multiple Linear Regression model with high multicollinearity?**\n",
        "\n",
        "  Ans. Here are several ways we can try to improve a model with high multicollinearity:\n",
        "\n",
        "  - Remove Highly Correlated Features:\n",
        "\n",
        "      - Identify the features that are highly correlated using a correlation matrix or by calculating the Variance Inflation Factor (VIF). A common rule of thumb is that VIF values above 5 or 10 indicate high multicollinearity.\n",
        "\n",
        "      - Remove one of the correlated features. The choice of which one to remove might depend on domain knowledge or which feature is theoretically less important. Be cautious, as removing a feature can lead to loss of information.\n",
        "\n",
        "  - Combine Correlated Features:\n",
        "\n",
        "      - If two or more features measure a similar underlying concept, we might be able to combine them into a single, more representative feature. This could involve averaging them, summing them, or using techniques like Principal Component Analysis (PCA) to create new, uncorrelated components that capture most of the variance. However, PCA can make the resulting features less interpretable.\n",
        "\n",
        "  - Collect More Data:\n",
        "\n",
        "      - Increasing the sample size can sometimes help to alleviate multicollinearity, as more data can provide a clearer picture of the independent effects of the predictors. However, this isn't always feasible.\n",
        "\n",
        "  - Center the Data:\n",
        "\n",
        "      - For polynomial terms or interaction terms created from existing features, centering the original features (subtracting the mean) before creating these new terms can sometimes reduce multicollinearity.\n",
        "\n",
        "  - Use Regularization Techniques:\n",
        "\n",
        "      - Ridge Regression (L2 regularization): Adds a penalty to the sum of the squared coefficients. This shrinks the coefficients towards zero, which can help stabilize them in the presence of multicollinearity, although it doesn't typically lead to feature elimination.\n",
        "\n",
        "      - Lasso Regression (L1 regularization): Adds a penalty to the sum of the absolute values of the coefficients. Lasso can perform feature selection by driving the coefficients of some variables to exactly zero, effectively removing them from the model.\n",
        "\n",
        "  - Partial Least Squares (PLS) Regression:\n",
        "\n",
        "      - PLS is a technique that can be used when we have many predictors and high multicollinearity. It aims to find components (like PCA) that both explain the variance in the predictors and have a high correlation with the response variable.\n",
        "---"
      ],
      "metadata": {
        "id": "ulApdJjyl9wr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. **What are some common techniques for transforming categorical variables for use in regression models?**\n",
        "\n",
        "  Ans. Some common techniques are:\n",
        "\n",
        "  - One-Hot Encoding:\n",
        "      - Creates binary (0 or 1) columns for each unique category in the variable.\n",
        "      - For a variable with k categories, it creates k new columns.\n",
        "      - Useful for nominal categorical variables (where there is no inherent order).\n",
        "      - Can lead to a high number of features if the categorical variable has many unique levels.\n",
        "\n",
        "  - Dummy Coding:\n",
        "      - Similar to one-hot encoding, but it creates k−1 binary columns for k categories.\n",
        "      - One category is treated as the reference category, and the coefficients of the other categories are interpreted relative to this reference.\n",
        "      - Helps to avoid multicollinearity issues that can arise with one-hot encoding if all k columns are included.\n",
        "\n",
        "  - Ordinal Encoding (Label Encoding):\n",
        "      - Assigns an integer to each unique category.\n",
        "      - Useful for ordinal categorical variables (where there is a meaningful order, e.g., \"low,\" \"medium,\" \"high\").\n",
        "      - The numerical assignment should respect this order.\n",
        "      - Can be problematic for nominal data as it implies an order that might not exist.\n",
        "\n",
        "  - Effect Coding (Sum Coding):\n",
        "      - Similar to dummy coding but uses values of 1, 0, and -1.\n",
        "      - For each category (except the reference), it assigns 1 if the observation belongs to that category, 0 otherwise.\n",
        "      - The reference category is coded as -1 across all the new columns.\n",
        "      - The intercept in the model represents the grand mean of the dependent variable.\n",
        "\n",
        "  - Binary Encoding:\n",
        "      - Converts each category into its binary representation.\n",
        "      - Then, each bit in the binary code becomes a new feature.\n",
        "      - Can be more space-efficient than one-hot encoding for high-cardinality categorical variables.\n",
        "---"
      ],
      "metadata": {
        "id": "EqWlMk5Pl-eb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. **What is the role of interaction terms in Multiple Linear Regression?**\n",
        "\n",
        "  Ans. The role of interaction terms is to model situations where the effect of one independent variable on the dependent variable depends on the value of another independent variable. In a standard multiple linear regression model, the effects of the predictors are assumed to be additive and independent. Interaction terms allow us to relax this assumption.\n",
        "---"
      ],
      "metadata": {
        "id": "7pJ6ZGpyl_UC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. **How can the interpretation of intercept differ between Simple and Multiple Linear Regression?**\n",
        "\n",
        "  Ans. Key Differences in Interpretation are:\n",
        "\n",
        "  - Conditioning on Other Variables: In multiple regression, the intercept's meaning is conditional on all other independent variables being zero. This is a crucial difference. In simple regression, the intercept is only in the context of the single independent variable.\n",
        "\n",
        "  - Practical Meaning:\n",
        "      - In simple regression, if X=0 is meaningful, the intercept often has a direct real-world interpretation (e.g., the baseline value of Y when X is absent).\n",
        "      - In multiple regression, it's less likely that all independent variables will meaningfully be zero at the same time. Therefore, the intercept might not have a direct, intuitive real-world interpretation. For example, if we're predicting house prices based on square footage and number of bedrooms, an intercept representing the price of a house with 0 square feet and 0 bedrooms is not practically meaningful.\n",
        "\n",
        "  - Impact of Included Variables: The value and interpretation of the intercept in a multiple regression model can change depending on which independent variables are included in the model. Adding or removing predictors can shift the point at which all predictors are zero, thus altering the intercept's value and its potential meaning.\n",
        "---\n"
      ],
      "metadata": {
        "id": "T4gzT0AImAgz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. **What is the significance of the slope in regression analysis, and how does it affect predictions?**\n",
        "\n",
        "  Ans. The slope in regression analysis is highly significant because it quantifies the relationship between an independent variable (your feature) and the dependent variable (the target).\n",
        "\n",
        "  Significance of the Slope:\n",
        "\n",
        "  - Direction of the Relationship:\n",
        "      - A positive slope indicates a positive relationship: as the independent variable increases, the dependent variable tends to increase.\n",
        "      - A negative slope indicates a negative relationship: as the independent variable increases, the dependent variable tends to decrease.\n",
        "      - A slope close to zero suggests a weak linear relationship between the variables.\n",
        "\n",
        "  - Magnitude of the Relationship:\n",
        "      - The absolute value of the slope indicates the strength of the linear relationship. A larger absolute slope means a steeper line, implying a more substantial change in the dependent variable for a unit change in the independent variable.\n",
        "\n",
        "  - Quantifying the Impact:\n",
        "      - The slope coefficient directly tells you how much the dependent variable is expected to change for a one-unit increase in the independent variable, holding all other variables constant in the case of multiple regression. This is crucial for understanding the \"effect size\" of a feature.\n",
        "    \n",
        "  How the Slope Affects Predictions:\n",
        "\n",
        "  The slope is a fundamental component of the regression equation, which is used to make predictions.\n",
        "\n",
        "  - In Simple Linear Regression (Y = β0 +β1X), if you have a new value of X, you multiply it by the slope (β1) and add the intercept (β0) to get the predicted value of Y. A larger (in magnitude) β1 means that changes in X will have a greater impact on the predicted Y.\n",
        "\n",
        "  - In Multiple Linear Regression (Y = β0 + β1X1 + β2X2 +...), each independent variable has its own slope (β1, β2, etc.). When making a prediction, the value of each independent variable is multiplied by its corresponding slope, and these products are summed along with the intercept. The slopes determine how much each feature contributes to the predicted outcome.\n",
        "---"
      ],
      "metadata": {
        "id": "ZiPkiVgWmBZp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. **How does the intercept in a regression model provide context for the relationship between variables?**\n",
        "\n",
        "  Ans. Here's how it provides context:\n",
        "\n",
        "  - Value When Predictors Are Zero: The intercept represents the predicted value of the dependent variable when all independent variables (our features) are equal to zero. This can be a meaningful baseline if a zero value for all predictors is within the realm of possibility and has a practical interpretation. For example, if we're modeling the price of a product based on its features, the intercept might represent a base price when all feature values are zero (though this might not always be realistic).\n",
        "\n",
        "  - Setting the Scale: The intercept helps set the overall scale of the predicted values. Even if the slopes indicate how much the target changes with each unit change in the predictors, the intercept determines the level from which these changes originate.\n",
        "\n",
        "  - Contextual Baseline: In some cases, the intercept can represent a natural baseline or a default value of the target variable in the absence of any influence from the predictors. For instance, in a model predicting plant growth based on fertilizer amount and sunlight hours, the intercept might represent the expected growth with no fertilizer and no sunlight (though practically, this might be zero or a minimal amount due to other factors not in the model).\n",
        "\n",
        "  - Influence on Predictions: While the slopes dictate how the predictors affect the target, the intercept ensures that the predictions are on the correct scale. A model with the correct slopes but a drastically wrong intercept would consistently over- or under-predict the target.\n",
        "---\n"
      ],
      "metadata": {
        "id": "hY7MQaDXmCLb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. **What are the limitations of using R² as a sole measure of model performance?**\n",
        "\n",
        "  Ans. The limitations are:\n",
        "\n",
        "  - R² Doesn't Indicate if the Model is Correct: A high R² only tells us that the model fits the data well, but it doesn't guarantee that the chosen model is the correct one for the underlying relationship. There might be a non-linear relationship that a linear model with a high R² still doesn't capture adequately. Always examine residual plots to check for patterns.\n",
        "\n",
        " - R² Increases with More Predictors: In Multiple Linear Regression, the R²\n",
        "  value will never decrease and usually increases when we add more independent variables to the model, even if those variables don't actually improve the model's predictive power. This can lead to overfitting, where the model fits the training data very well but performs poorly on new, unseen data.\n",
        "\n",
        "  - R² Doesn't Tell You About Bias: A high R² doesn't mean the model's predictions are unbiased. The model could be consistently over- or under-predicting, even with a high R². We need to look at other metrics and the nature of the errors.\n",
        "\n",
        "  - R² is Sensitive to Outliers: Outliers can disproportionately influence the R² value, potentially giving a misleading impression of the model's fit.\n",
        "\n",
        "  - R² Doesn't Imply Causation: A high R² indicates a strong correlation but does not imply that changes in the independent variables cause changes in the dependent variable.\n",
        "---"
      ],
      "metadata": {
        "id": "NLxLDWmnmDAS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. **How would you interpret a large standard error for a regression coefficient?**\n",
        "\n",
        "  Ans. A large standard error for a regression coefficient is a signal that the estimate of that coefficient is imprecise or unreliable. Here's how I would interpret it:\n",
        "\n",
        "  - Uncertainty in the Coefficient Estimate: A large standard error means that there's a wide range of plausible values for the true coefficient. The larger the standard error relative to the coefficient itself, the less confident we are about the actual magnitude and even the direction (positive or negative) of the effect of that feature on the target variable.\n",
        "\n",
        "  - Lower Statistical Significance: The standard error is in the denominator of the t-statistic (for individual coefficients) used in hypothesis testing. A large standard error will result in a smaller absolute t-value, making it less likely that the coefficient will be found to be statistically significantly different from zero (i.e., we can't confidently say the feature has a real effect).\n",
        "\n",
        "  - Potential Issues with the Data or Model: A large standard error can arise due to several reasons:\n",
        "      - Small Sample Size: With fewer data points, it's harder to get a precise estimate of the relationship.\n",
        "      - High Multicollinearity: If the feature is highly correlated with other predictors in a multiple regression model, it can inflate the standard errors of their coefficients. The model struggles to disentangle the individual effects of the correlated features.\n",
        "      - Low Variability in the Predictor: If the independent variable has very little variation, it's hard to accurately estimate its impact on the dependent variable.\n",
        "      - Model Misspecification: While less direct, if the model doesn't correctly capture the underlying relationship, it can lead to larger standard errors.\n",
        "---"
      ],
      "metadata": {
        "id": "0bJqkNJsmD6B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. **How can heteroscedasticity be identified in residual plots, and why is it important to address it?**\n",
        "\n",
        "  Ans. Identifying Heteroscedasticity in Residual Plots:\n",
        "\n",
        "  In a plot of residuals (on the y-axis) versus the predicted values (on the x-axis), or against each independent variable, we should ideally see a random scatter of points with no discernible pattern. If heteroscedasticity is present, we might observe patterns such as:\n",
        "  - Funnel Shape: The spread (variance) of the residuals increases or decreases as the predicted values increase. This is a common indicator. For example, the residuals might be tightly clustered on the left side of the plot and more spread out on the right (or vice versa).\n",
        "  - Cone Shape: Similar to a funnel, but the spread might start narrow and widen (or widen and then narrow) more distinctly.\n",
        "  - Other Non-Constant Variance: Any pattern where the vertical spread of the points is not roughly the same across all values on the x-axis suggests heteroscedasticity.\n",
        "\n",
        "  Why is it Important to Address Heteroscedasticity:\n",
        "\n",
        "  Ignoring heteroscedasticity can lead to several problems that affect the reliability of your regression model and the conclusions we draw about our features:\n",
        "  - Inefficient Coefficient Estimates: While the Ordinary Least Squares (OLS) coefficient estimates remain unbiased, they are no longer the Best Linear Unbiased Estimators (BLUE). This means there could be other unbiased estimators with smaller variance.\n",
        "  - Unreliable Standard Errors: OLS typically underestimates the standard errors of the coefficients when heteroscedasticity is present. This leads to:\n",
        "      - Inflated t-statistics: Smaller standard errors result in larger t-values.\n",
        "      - Deflated p-values: Larger t-values make it easier to reject the null hypothesis.\n",
        "      - Incorrect conclusions about significance: We might incorrectly conclude that a feature is statistically significant.\n",
        "\n",
        "  - Invalid Statistical Inferences: Because the standard errors are wrong, confidence intervals for the coefficients will also be unreliable.\n",
        "\n",
        "  - Suboptimal Predictions: Although the predictions themselves might not be biased, the uncertainty associated with those predictions (based on the standard errors) will be incorrect.\n",
        "---\n"
      ],
      "metadata": {
        "id": "_eecGyBpmE6j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. **What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?**\n",
        "\n",
        "  Ans. Here's what a high R² but a noticeably lower adjusted R² suggests:\n",
        "\n",
        "\n",
        "  - High R²: The model explains a large proportion of the variance in the dependent variable within the training data. It indicates a good fit to the data used to build the model.\n",
        "\n",
        "  - Lower Adjusted R²: The adjusted R² takes into account the number of predictors in the model. It penalizes the addition of variables that do not significantly improve the model's fit. A lower adjusted R² compared to the R² indicates that some of the added predictors (your features) are not contributing much to explaining the variance in the target variable. In fact, they might be fitting the noise in the training data rather than the underlying signal.\n",
        "---"
      ],
      "metadata": {
        "id": "3rjsf4Z_mF_p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. **Why is it important to scale variables in Multiple Linear Regression?**\n",
        "\n",
        "  Ans. Scaling variables in Multiple Linear Regression can be important for several reasons, although it's not always strictly necessary for the mathematical solution of the model. Here's why it's often a good practice:\n",
        "\n",
        "  - Preventing Features with Larger Scales from Dominating: If our features have vastly different scales (e.g., one ranging from 0 to 1, and another from 0 to 1,000,000), the feature with the larger scale might unduly influence the model's coefficients simply due to its magnitude, not necessarily its predictive power. Scaling brings them to a comparable range.\n",
        "\n",
        "  - Improving the Performance of Optimization Algorithms: If we're using gradient descent to find the optimal coefficients (which is common with large datasets), scaling can help the algorithm converge faster. Features on different scales can lead to an elongated cost function, making it take longer for gradient descent to find the minimum.\n",
        "\n",
        "  - Facilitating Interpretation of Coefficients (when standardized): If we standardize your features (e.g., using Z-score scaling so they have a mean of 0 and a standard deviation of 1), the magnitudes of the resulting regression coefficients become more directly comparable. A larger absolute standardized coefficient then indicates a stronger effect of that feature (in terms of standard deviations) on the target variable.\n",
        "\n",
        "  - Regularization: When we use regularization techniques like Ridge or Lasso regression, scaling becomes crucial. These methods penalize the size of the coefficients. Without scaling, features with larger original scales would be penalized more heavily, which might not reflect their true importance.\n",
        "\n",
        "  - Reducing Multicollinearity (in specific cases): When we create polynomial or interaction terms from your original features, scaling (especially centering) can help reduce the multicollinearity that might arise.\n",
        "---"
      ],
      "metadata": {
        "id": "40BUBka_mH_D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. **What is polynomial regression?**\n",
        "\n",
        "  Ans. Polynomial Regression is a form of linear regression in which the relationship between the independent variable (X) and the dependent variable (Y) is modeled as an nth degree polynomial. Although the relationship between X and Y is non-linear, polynomial regression fits a linear model to the transformed features.\n",
        "---"
      ],
      "metadata": {
        "id": "0ykYwyNumJhb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. **How does polynomial regression differ from linear regression?**\n",
        "\n",
        "  Ans. Here's how polynomial regression differs from standard linear regression:\n",
        "\n",
        "  - Linear Regression:\n",
        "      - Relationship: Assumes a linear relationship between the independent variable(s) and the dependent variable. This means the model tries to fit a straight line (in simple linear regression) or a hyperplane (in multiple linear regression) through the data.\n",
        "      - Equation (Simple): Y = β0 + β1X + ϵ\n",
        "      - Use Case: Suitable when the relationship between variables appears to be a straight line.\n",
        "\n",
        "  - Polynomial Regression:\n",
        "      - Relationship: Models a non-linear relationship between the independent variable(s) and the dependent variable by fitting a polynomial equation to the data. It can capture curves.\n",
        "      - Equation (Simple, degree d): Y = β0 + β1X + β2X² +...+ βdX^d + ϵ\n",
        "      - Feature Transformation: We can think of it as implicitly engineering new features that are powers of the original feature(s). For example, if we have a single feature X and we use a polynomial of degree 2, we are essentially using X and X2 as our predictors in a linear model.\n",
        "      - Use Case: Suitable when the relationship between variables appears to be curved. For example, the relationship between the age of a tree and its height might be non-linear initially.\n",
        "---"
      ],
      "metadata": {
        "id": "s48vcbh2mK2x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. **When is polynomial regression used?**\n",
        "\n",
        "  Ans. Polynomial regression is commonly used in the following scenarios:\n",
        "\n",
        "  - Curvilinear Data: When a scatter plot of our data shows a curve rather than a straight line, polynomial regression can often provide a better fit.\n",
        "\n",
        "  - Modeling Physical Processes: Many physical phenomena have non-linear relationships. For example:\n",
        "      - The trajectory of a projectile.\n",
        "      - The growth rate of a plant over time.\n",
        "      - The relationship between temperature and the rate of a chemical reaction.\n",
        "\n",
        "  - Feature Engineering to Capture Non-Linearities: Even if the underlying model we ultimately use is linear (like a linear model in a neural network), we might use polynomial regression as an initial step to understand and model non-linear relationships present in our raw features. We could then decide to include these polynomial features in our final model.\n",
        "\n",
        "  - Approximating Complex Functions: Polynomials can be used to approximate more complex non-linear functions over a certain range.\n",
        "---"
      ],
      "metadata": {
        "id": "zIpUOrHhmM1d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. **What is the general equation for polynomial regression?**\n",
        "\n",
        "  Ans. The general equation for polynomial regression with a single independent variable X and a degree n is:\n",
        "\n",
        "  Y = β0 + β1X + β2X² + β3X³ +...+ βnX^n + ϵ\n",
        "\n",
        "  Where:\n",
        "\n",
        "  - Y is the dependent variable.\n",
        "  - X is the independent variable.\n",
        "  - β0, β1, β2,..., βn are the coefficients to be estimated.\n",
        "  - n is the degree of the polynomial.\n",
        "  - ϵ is the error term.\n",
        "---\n"
      ],
      "metadata": {
        "id": "dBmdRtipmOV8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. **Can polynomial regression be applied to multiple variables?**\n",
        "\n",
        "  Ans. For multiple independent variables, the equation becomes more complex, involving polynomial terms and interaction terms of those variables. For instance, with two independent variables (X1 and X2) and a degree of 2, the equation would be:\n",
        "\n",
        "  Y = β0 + β1X1 + β2X2 + β3X1² + β4X2² + β5X1X2 + ϵ\n",
        "\n",
        "  Here, we not only have the squared terms of each individual variable but also an interaction term (X1X2).\n",
        "---"
      ],
      "metadata": {
        "id": "Rv_lN-KlmQy2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. **What are the limitations of polynomial regression?**\n",
        "\n",
        "  Ans. While polynomial regression is a powerful tool for modeling non-linear relationships, it comes with its own set of limitations:\n",
        "\n",
        "  - Overfitting: This is a significant risk, especially with higher-degree polynomials. A high-degree polynomial can fit the training data very closely, including the noise, leading to poor generalization on unseen data. We might see a model that looks great on our training set (R² close to 1) but performs poorly on a test set.\n",
        "\n",
        "  - Interpretability: As the degree of the polynomial increases, the model becomes harder to interpret. In simple linear regression, the coefficient directly tells us the change in the target for a unit change in the feature. With polynomial terms (like X², X³), the relationship becomes more complex, and the individual coefficients are less intuitively meaningful in isolation.\n",
        "\n",
        "  - Extrapolation: Polynomial regression models can produce unreliable predictions when extrapolating beyond the range of the training data. The polynomial curve can take unexpected turns outside the observed data range.\n",
        "\n",
        "  - Ill-conditioning and Multicollinearity: Higher powers of a feature can be highly correlated with lower powers of the same feature (e.g., X and X²). This multicollinearity can destabilize the coefficient estimates and make the model sensitive to small changes in the data. Ill-conditioning of the design matrix can also occur, especially with high-degree polynomials and a narrow range of X values, leading to inaccurate parameter estimation.\n",
        "\n",
        "  - Choosing the Degree: Selecting the appropriate degree of the polynomial is not always straightforward. A degree that is too low might underfit the data, while one that is too high might overfit. Techniques like cross-validation can help, but it adds complexity to the model selection process.\n",
        "---"
      ],
      "metadata": {
        "id": "sdggqNermR_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. **What methods can be used to evaluate model fit when selecting the degree of a polynomial?**\n",
        "\n",
        "  Ans. Here are several methods we can use to evaluate model fit and help select the appropriate degree:\n",
        "\n",
        "  - Visual Inspection of the Fitted Curve:\n",
        "      - Plot the fitted polynomial curve against the actual data points. This can give us a qualitative sense of how well the model captures the underlying trend. Look for a curve that follows the data without being overly wiggly or fitting the noise too closely.\n",
        "\n",
        "  - R² and Adjusted R²:\n",
        "      - R² : Generally increases as we increase the degree of the polynomial, so it's not the best sole indicator as it doesn't penalize complexity.\n",
        "      - Adjusted R²: This metric adjusts the R² for the number of predictors (in this case, the polynomial terms). It will increase only if the addition of a new term improves the model more than expected by chance. A decrease in adjusted R² when increasing the degree suggests that the added terms are not beneficial. We'd look for a degree where the adjusted R² starts to plateau or decrease.\n",
        "\n",
        "  - Analysis of Residuals:\n",
        "      - Plot the residuals (the differences between the predicted and actual values) against the independent variable or the predicted values.\n",
        "      - For a good fit, the residuals should be randomly scattered around zero, showing no clear patterns. Patterns in the residuals (like a curve) might indicate that a higher-degree polynomial could be a better fit, or that the model is systematically missing some structure in the data.\n",
        "\n",
        "  - Cross-Validation:\n",
        "      - This is a robust method to estimate how well the model will generalize to unseen data.\n",
        "      - We can try different polynomial degrees and use k-fold cross-validation to evaluate the performance (e.g., using Mean Squared Error or R²) on the validation folds.\n",
        "      - Choose the degree that gives the best cross-validation performance. This helps in selecting a model that generalizes well.\n",
        "\n",
        "  - Information Criteria (AIC, BIC):\n",
        "      - These are statistical measures that assess the trade-off between the goodness of fit and the complexity of the model (number of parameters).\n",
        "      - Lower values of AIC (Akaike Information Criterion) or BIC (Bayesian Information Criterion) generally indicate a better model. These criteria penalize models with more parameters.\n",
        "---"
      ],
      "metadata": {
        "id": "BpYw2xALmTVE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. **Why is visualization important in polynomial regression?**\n",
        "\n",
        "  Ans. Visualization is particularly important in polynomial regression for several key reasons:\n",
        "\n",
        "  - Understanding the Non-Linear Relationship: Unlike linear regression where a scatter plot and the fitted line are often sufficient, polynomial regression models curves. Visualizing the fitted polynomial curve overlaid on the scatter plot of the data is crucial to see how well the model captures the non-linear trend. It helps us to intuitively assess if the degree of the polynomial is appropriate - is it following the data's shape, or is it too wiggly and potentially overfitting?\n",
        "\n",
        "  - Diagnosing Overfitting and Underfitting:\n",
        "      - Overfitting: A high-degree polynomial might pass through almost every data point, resulting in a very complex and potentially erratic curve. Visualization can immediately reveal this \"over-flexibility,\" which is a hallmark of overfitting.\n",
        "      - Underfitting: Conversely, if we use a low-degree polynomial for a highly non-linear dataset, the fitted curve might be too simplistic and fail to capture the underlying patterns. Visualization will show a poor fit where the curve deviates significantly from the data's trend.\n",
        "\n",
        "  - Evaluating Model Fit Qualitatively: While metrics like R² and MSE provide quantitative measures, visualization offers a qualitative assessment. We can see where the model is doing a good job of fitting the data and where it's not. This can provide insights that numbers alone might miss. For example, the overall R² might be okay, but a plot could reveal that the model fits well in one region of the data but poorly in another.\n",
        "\n",
        "  - Guiding the Choice of Polynomial Degree: By plotting models of different degrees (e.g., degree 2, 3, 4), we can visually compare how well each fits the data and make a more informed decision about the appropriate degree to use. We're looking for a balance where the curve follows the trend without being overly complex.\n",
        "\n",
        "  - Checking Assumptions (Indirectly): While polynomial regression doesn't have the same strict linearity assumption as simple linear regression with the original features, visualizing the fit can help you see if the chosen polynomial degree seems reasonable for the underlying relationship, or if there might be other issues not captured by a polynomial.\n",
        "---\n"
      ],
      "metadata": {
        "id": "pMM12lfXmUGM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. **How is polynomial regression implemented in Python?**\n",
        "\n",
        "  Ans. Implementing polynomial regression in Python is quite straightforward using libraries like scikit-learn. Here's a common approach:\n",
        "\n",
        "  - Generate Polynomial Features: First, we'll use scikit-learn's PolynomialFeatures transformer to create new features that are the polynomial combinations of our original features. For a single feature X, if we specify a degree of n, it will generate new features for X1, X2,..., Xn. If we have multiple features, it will also generate interaction terms.\n",
        "\n",
        "  - Train a Linear Regression Model: Once we have these polynomial features, we can train a standard LinearRegression model on them. The model will learn the coefficients for each of these polynomial terms.\n",
        "\n",
        "  Here's a simple example with one independent variable:"
      ],
      "metadata": {
        "id": "rDxzR2E1mVHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Sample data (non-linear relationship)\n",
        "np.random.seed(0)\n",
        "X = np.sort(5 * np.random.rand(100, 1), axis=0)\n",
        "y = np.sin(X).ravel() + np.random.normal(0, 0.1, X.shape[0])\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Choose the degree of the polynomial\n",
        "degree = 3\n",
        "\n",
        "# Create polynomial features\n",
        "poly = PolynomialFeatures(degree=degree)\n",
        "X_train_poly = poly.fit_transform(X_train)\n",
        "X_test_poly = poly.transform(X_test)\n",
        "\n",
        "# Train a linear regression model\n",
        "model = LinearRegression()\n",
        "model.fit(X_train_poly, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test_poly)\n",
        "\n",
        "# Evaluate the model\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Mean Squared Error: {mse}\")\n",
        "\n",
        "# Visualize the results\n",
        "plt.scatter(X_test, y_test, color='blue', label='Actual')\n",
        "# Generate points for the polynomial curve\n",
        "X_plot = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)\n",
        "X_plot_poly = poly.transform(X_plot)\n",
        "y_plot = model.predict(X_plot_poly)\n",
        "plt.plot(X_plot, y_plot, color='red', label=f'Polynomial Regression (degree {degree})')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('y')\n",
        "plt.title('Polynomial Regression')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "e0t3MYiFgwMm",
        "outputId": "5990833b-6102-4ddd-fb9e-0b5e70ba0b6a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 0.012745334855089843\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}